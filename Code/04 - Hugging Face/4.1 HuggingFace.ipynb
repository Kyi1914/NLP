{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your go-to tool for using any pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.38.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate # metrics \n",
    "evaluate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.27.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic thing in Huggingface; you insert the pretrained model, and just use it for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998350143432617}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "clf(\"I love hugging face so much\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0191cf9d1a46be9b63618eced2d0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64488f9f520f4de2a789e35d76edaeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc3a2c3535f4157a8d78d15f245365a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e710790c264210b9a53341ff1056d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdd366ec7ea4745aa4d2c0230bd9494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is NLP course on Huggingface',\n",
       " 'labels': ['education', 'tech', 'sports'],\n",
       " 'scores': [0.6124476194381714, 0.3685862720012665, 0.018966125324368477]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = pipeline(\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n",
    "# m = multilanguage, nli = natural language inference # bart >> encoder+decoder\n",
    "clf(\"This is NLP course on Huggingface\", candidate_labels = ['education', 'tech', 'sports'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "education > highest probability (0.61)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdc3a97d55543f0b2cdc824fd0d2ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d81b7d3fb154f6f96afdba16f1b5bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa063c77b2da4b878ac5fc7a9aa329e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465e54ea6329428bb1203f0afec4d4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ab1aeed94b48d7a334fd45713737a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9264afb4024a0d84ca41c233c76e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf45b5b9e8174aa380dcd623077f0721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'AI is transforming our everyday lives and creating a new kind of entertainment. Our children are learning and learning to love, care for and care for others, support, and support our family.\\n\\n\\n\\n\\n\\n\\n\\n\\nAll is one for all.\\n'},\n",
       " {'generated_text': \"AI is transforming our everyday lives, both economic, physical and life, as we are able to do because of our unique skill sets. It's time that we start to work smarter and more effectively at work.\\n\\n\\n\\n\\n[email protected]\\n\\nFor more information about PAMPED in the UK please visit www.pamPAPED.co.uk. Your views are welcome.\\nImage: Image from Flickr.\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = pipeline('text-generation', model = \"distilgpt2\")\n",
    "# gpt2 >> from openAI\n",
    "# distil >> learn the label from gpt2 >> this process is called distillation\n",
    "gen(\"AI is transforming our everyday lives\", max_length = 100, num_return_sequences = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14332418143749237,\n",
       "  'token': 2239,\n",
       "  'token_str': ' learning',\n",
       "  'sequence': 'Chaky loves to teach deep learning'},\n",
       " {'score': 0.0965745821595192,\n",
       "  'token': 9589,\n",
       "  'token_str': ' breathing',\n",
       "  'sequence': 'Chaky loves to teach deep breathing'},\n",
       " {'score': 0.0719672366976738,\n",
       "  'token': 41711,\n",
       "  'token_str': ' breaths',\n",
       "  'sequence': 'Chaky loves to teach deep breaths'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = pipeline('fill-mask',  model = 'distilroberta-base')\n",
    "# roberta - base on BERT, bassically train on masking; adding more layers to bert\n",
    "# distil - distil version of roberta\n",
    "mlm('Chaky loves to teach deep <mask>', top_k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ead71f7c1a4f93be741ee2a6a13e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bd2e2127524e46b63dcd34654ce7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded66473cf5b486c9cf2492492db439a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02376756160c4f7e982c2c8f66b2ed90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb4b2e9471f4be3b4cf9a8f972cff3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9753952622413635, 'start': 40, 'end': 43, 'answer': 'AIT'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = pipeline('question-answering', model = 'distilbert-base-cased-distilled-squad')\n",
    "# squad = famous dataset for qa\n",
    "qa(question = \"Where do Chaky work?\", context = \"My name is Chaky and I love to teach at AIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08593502640724182,\n",
       "  'token': 37171,\n",
       "  'token_str': ' courier',\n",
       "  'sequence': 'This man works as a courier'},\n",
       " {'score': 0.08269880712032318,\n",
       "  'token': 28894,\n",
       "  'token_str': ' translator',\n",
       "  'sequence': 'This man works as a translator'},\n",
       " {'score': 0.05225023254752159,\n",
       "  'token': 38233,\n",
       "  'token_str': ' waiter',\n",
       "  'sequence': 'This man works as a waiter'},\n",
       " {'score': 0.051457736641168594,\n",
       "  'token': 8298,\n",
       "  'token_str': ' consultant',\n",
       "  'sequence': 'This man works as a consultant'},\n",
       " {'score': 0.037423163652420044,\n",
       "  'token': 33080,\n",
       "  'token_str': ' bartender',\n",
       "  'sequence': 'This man works as a bartender'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = pipeline(\"fill-mask\", model = 'distilroberta-base')\n",
    "result = mlm(\"This man works as a <mask>\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10744372010231018,\n",
       "  'token': 35698,\n",
       "  'token_str': ' waitress',\n",
       "  'sequence': 'This woman works as a waitress'},\n",
       " {'score': 0.08695955574512482,\n",
       "  'token': 28894,\n",
       "  'token_str': ' translator',\n",
       "  'sequence': 'This woman works as a translator'},\n",
       " {'score': 0.06901882588863373,\n",
       "  'token': 9008,\n",
       "  'token_str': ' nurse',\n",
       "  'sequence': 'This woman works as a nurse'},\n",
       " {'score': 0.06353699415922165,\n",
       "  'token': 36289,\n",
       "  'token_str': ' prostitute',\n",
       "  'sequence': 'This woman works as a prostitute'},\n",
       " {'score': 0.04852951318025589,\n",
       "  'token': 33080,\n",
       "  'token_str': ' bartender',\n",
       "  'sequence': 'This woman works as a bartender'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = mlm(\"This woman works as a <mask>\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tha's for the Idea of Pipeline.\n",
    "in Hugging Face:\n",
    "- search pipeline and models that it support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" # declare the model we want: distilbert and finetuned based on sst-2-english\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint) # we will use the same tokenizer used in the pretrain model (checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = ['Chaky has been waiting in queue for sushi',\n",
    "              'Huggingface can do lots of stuffs so make sure you try everything']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15775,  4801,  2038,  2042,  3403,  1999, 24240,  2005, 10514,\n",
       "          6182,   102,     0,     0,     0,     0],\n",
       "        [  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
       "          2191,  2469,  2017,  3046,  2673,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors ='pt') # pt = pytorch, ts = tensor\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] chaky has been waiting in queue for sushi [SEP] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  101, 15775,  4801,  2038,  2042,  3403,  1999, 24240,  2005, 10514,\n",
    "          6182,   102,     0,     0,     0,     0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] huggingface can do lots of stuffs so make sure you try everything [SEP]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
    "          2191,  2469,  2017,  3046,  2673,   102])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "you can see for the fist data: 0,0,0,0 >> they are padding, so no need to pay attention. Ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second component of Pipeline(after tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15775,  4801,  2038,  2042,  3403,  1999, 24240,  2005, 10514,\n",
       "          6182,   102,     0,     0,     0,     0],\n",
       "        [  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
       "          2191,  2469,  2017,  3046,  2673,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send input to the model\n",
    "\n",
    "outputs = model(**inputs) # tell the model that **: input is a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.0646,  0.2758, -0.3721,  ...,  0.0342, -0.4803,  0.3824],\n",
       "         [-0.1979,  0.3366,  0.4101,  ...,  0.2438,  0.1842,  0.1367],\n",
       "         [-0.6114,  0.0397,  0.6715,  ...,  0.1152,  0.1026, -0.0419],\n",
       "         ...,\n",
       "         [ 0.0606,  0.0074,  0.0046,  ...,  0.2112, -0.4586,  0.3855],\n",
       "         [-0.2745,  0.3977, -0.0904,  ...,  0.1042, -0.3791,  0.2757],\n",
       "         [ 0.0499,  0.0522,  0.0142,  ...,  0.2213, -0.4748,  0.3702]],\n",
       "\n",
       "        [[ 0.5644,  0.3998,  0.4761,  ...,  0.3903,  0.7795, -0.3462],\n",
       "         [ 0.1141,  0.4574,  1.0063,  ...,  0.3859,  0.6240,  0.0430],\n",
       "         [ 0.3260,  0.3940,  1.1281,  ...,  0.3320,  0.6197, -0.2190],\n",
       "         ...,\n",
       "         [ 0.3110,  0.5700,  0.3175,  ...,  0.3519,  1.0604, -0.9748],\n",
       "         [ 0.4691,  0.2198,  0.2427,  ...,  0.3746,  0.9758, -0.6325],\n",
       "         [ 0.8492,  0.2319,  0.1885,  ...,  0.6911,  0.5335, -0.6505]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0646,  0.2758, -0.3721,  ...,  0.0342, -0.4803,  0.3824],\n",
       "         [-0.1979,  0.3366,  0.4101,  ...,  0.2438,  0.1842,  0.1367],\n",
       "         [-0.6114,  0.0397,  0.6715,  ...,  0.1152,  0.1026, -0.0419],\n",
       "         ...,\n",
       "         [ 0.0606,  0.0074,  0.0046,  ...,  0.2112, -0.4586,  0.3855],\n",
       "         [-0.2745,  0.3977, -0.0904,  ...,  0.1042, -0.3791,  0.2757],\n",
       "         [ 0.0499,  0.0522,  0.0142,  ...,  0.2213, -0.4748,  0.3702]],\n",
       "\n",
       "        [[ 0.5644,  0.3998,  0.4761,  ...,  0.3903,  0.7795, -0.3462],\n",
       "         [ 0.1141,  0.4574,  1.0063,  ...,  0.3859,  0.6240,  0.0430],\n",
       "         [ 0.3260,  0.3940,  1.1281,  ...,  0.3320,  0.6197, -0.2190],\n",
       "         ...,\n",
       "         [ 0.3110,  0.5700,  0.3175,  ...,  0.3519,  1.0604, -0.9748],\n",
       "         [ 0.4691,  0.2198,  0.2427,  ...,  0.3746,  0.9758, -0.6325],\n",
       "         [ 0.8492,  0.2319,  0.1885,  ...,  0.6911,  0.5335, -0.6505]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape\n",
    "# ([batch_size, seq_len, hidden_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step of Pipeline(after the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.5754, -2.1154],\n",
       "        [-2.8534,  2.8917]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5754, -2.1154],\n",
       "        [-2.8534,  2.8917]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9909, 0.0091],\n",
       "        [0.0032, 0.9968]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "{0: 'NEGATIVE', 1: 'POSITIVE'} \n",
    "\n",
    "out sentence: \n",
    "\n",
    "raw_inputs = ['Chaky has been waiting in queue for sushi',\n",
    "              'Huggingface can do lots of stuffs so make sure you try everything']\n",
    "\n",
    "result from the model: \n",
    "\n",
    "tensor([[0.9909, 0.0091],\n",
    "        [0.0032, 0.9968]]\n",
    "\n",
    "The first sentence is negative as 0.99 > 0.0091 [0 > 1]  \n",
    "the second sentence is positive as 0.003 < 0.99 [0 < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
