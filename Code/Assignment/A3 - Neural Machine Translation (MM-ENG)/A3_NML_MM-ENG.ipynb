{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 - Neural Machine Translation (Myanmar to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm # progress bar\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 18088\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1019\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "    num_rows: 18088\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNT.URLID': '87564',\n",
       " 'SNT.URLID.SNTID': '13',\n",
       " 'url': 'http://en.wikinews.org/wiki/Data_for_3_million_UK_driving_candidates_lost',\n",
       " 'translation': {'bg': 'অক্টোবরে এইচএম রেভিনিউ ২৫ মিলিয়ন লোকের তথ্য হারিয়ে ফেলার পরে এটিই হল ইউকে-তে প্রথম এত বড় তথ্যের ক্ষতি।',\n",
       "  'en': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October.',\n",
       "  'en_tok': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October .',\n",
       "  'fil': 'Ito ang unang malakihang pagkawala ng data sa UK dahil ang impormasyon sa 25 milyong tao ay nawalan ng HM Revenue noong Oktubre.',\n",
       "  'hi': 'यह ब्रिटेन में डेटा का पहला बड़ा नुकसान है क्योंकि अक्टूबर में HM रेवेन्यू द्वारा 25 मिलियन लोगों की जानकारी गुम हो गई थी।',\n",
       "  'id': 'Ini adalah kehilangan data yang besar pertama di UK sejak hilangnya informasi tentang 25 juta orang oleh HM Revenue di bulan Oktober.',\n",
       "  'ja': 'これは、10月に歳入関税庁が2500万人分の情報を失って以来初めてのイギリスでの大きなデータ紛失である。',\n",
       "  'khm': 'នេះជាការបាត់បង់ទិន្នន័យដ៏ធំំលើកទីមួយក្នុងចក្រភពអង់គ្លេសបន្ទាប់ពីបានបាត់បង់ព័ត៌មាន25លាននាក់នៅទីភ្នាក់ងារចំណូលគយនៅខែតុលា។',\n",
       "  'lo': 'ນີ້ແມ່ນການສູນເສຍຄັ້ງສຳຄັນຂອງຂໍ້ມູນໃນ ສະຫະລາຊະອານາຈັກ ຕັ້ງແຕ່ຂໍ້ມູນຂອງ 25 ລ້ານຄົນ ໄດ້ສູນຫາຍໄປໂດຍ HM ເຮເວີນູ ໃນເດືອນຕຸລາ.',\n",
       "  'ms': 'Ini adalah kehilangan data terbesar di UK semenjak maklumat 25 juta orang dihilangkan oleh HM Revenue pada Oktober.',\n",
       "  'my': 'အဆိုပါကိစ္စ သည် အောက်တိုဘာလ တွင် လူ ၂၅ သန်း ၏ သတင်းအချက်အလက်များ အခွန်ဝန်ကြီးဌာန ကြောင့် ဆုံးရှုံးခဲ့ ကတည်းက ဗြိတိန်နိုင်ငံ တွင် ပထမဦးဆုံးသော အဓိကကျသည့် အချက်အလက်များ ၏ ပျောက်ဆုံးမှု ဖြစ်သည် ။',\n",
       "  'th': 'นับเป็นการสูญหายของข้อมูลครั้งใหญ่ครั้งแรกในUK ตั้งแต่ข้อมูลของประชาชน25 ล้านคนสูญหายโดยHMของสรรพากรเมื่อเดือนตุลาคม',\n",
       "  'vi': 'Đây là vụ mất dữ liệu lớn đầu tiên tại Anh kể từ khi Cục Thuế làm mất dữ liệu của 25 triệu người vào tháng Mười.',\n",
       "  'zh': '这是自去年10月英国税务机构丢失2500万人信息以来，英国首次出现重大数据丢失。'}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bg': 'গতকাল ১৭৩০ ইউটিসি-তে হাউস অফ্\\u200c কমন্\\u200cস্\\u200c-এ ইউকে-র পরিবহন সচিব রুথ কেলি এই তথ্যগুলি দিয়েছেন।',\n",
       " 'en': 'Details were given by the UK Transport Secretary, Ruth Kelly, in the House of Commons at 1730 UTC yesterday.',\n",
       " 'en_tok': 'Details were given by the UK Transport Secretary , Ruth Kelly , in the House of Commons at 1730 UTC yesterday .',\n",
       " 'fil': 'Ang mga detalye ay ibinigay ng UK transport Secretary, na si Ruth Kelly, sa House of Commons sa ika-17:30 UTC kahapon.',\n",
       " 'hi': 'कल ब्रिटेन के परिवहन सचिव रूथ केली द्वारा 1730 UTC पर हाउस ऑफ़ कॉमन्स में विवरण दिए गए।',\n",
       " 'id': 'Detil diberikan oleh Sekretaris Kementerian Transportasi UK, Ruth Kelly, di Dewan Perwakilan Rakyat kemarin 17:30 UTC.',\n",
       " 'ja': '詳細は昨日UTC17時30分、英国議会でイギリスのルス・ケリー運輸大臣によって伝えられた。',\n",
       " 'khm': 'ព័ត៌មានលំអិតត្រូវបានផ្តល់ដោយរដ្ឋមន្ត្រីដឹកជញ្ចូន លោករ៉ូថ ខេលលី នៅក្នុងសភានៅម៉ោង1730ម្សិលមិញ។',\n",
       " 'lo': 'ຂໍ້ມູນໄດ້ຖືກສະໜອງໂດຍ ເລຂາທິການຂົນສົ່ງ ສະຫະລາຊະອານາຈັກ ຣູດ ເຄລີ່ ໃນສະພາຕໍ່າ ທີ່ 1730 UTC ມື້ວານນີ້.',\n",
       " 'ms': 'Butiran telah diberikan oleh Setiausaha Pengangkutan UK, Ruth Kelly, di Dewan Rakyat pada 1730 UTC semalam.',\n",
       " 'my': 'အသေးစိတ်များ ကို မနေ့က ၁၇၃၀ ယူတီစီ ၌ အောက်လွှတ်တော် ရှိ ဗြိတိန်နိုင်ငံ ပို့ဆောင်ရေး အတွင်းရေးမှူး ရုသ်ကယ်လီ က ပေးခဲ့သည် ။',\n",
       " 'th': 'รายละเอียดถูกเปิดเผยโดยเลขาธิการกระทรวงคมนาคมของUK นางRuth Kelly ในสภาสามัญชน เมื่อเวลา17:30 UTC',\n",
       " 'vi': 'Thông tin chi tiết được cung cấp bởi Bộ trưởng Giao thông Anh, Ruth Kelly, tại Hạ nghị viện ngày hôm qua lúc 17:30 theo Giờ Phối hợp Quốc tế.',\n",
       " 'zh': '英国交通大臣露丝·凯利昨天于英国时间17：30分在下议院公布了详细情况。'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Details were given by the UK Transport Secretary, Ruth Kelly, in the House of Commons at 1730 UTC yesterday.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'အသေးစိတ်များ ကို မနေ့က ၁၇၃၀ ယူတီစီ ၌ အောက်လွှတ်တော် ရှိ ဗြိတိန်နိုင်ငံ ပို့ဆောင်ရေး အတွင်းရေးမှူး ရုသ်ကယ်လီ က ပေးခဲ့သည် ။'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']['my']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset that will contain only Myanmar language as a target language and English language as a source language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetENMY = {}\n",
    "\n",
    "# Define source and target languages\n",
    "SRC_LANGUAGE = 'en'  # Source language is English\n",
    "TRG_LANGUAGE = 'my'  # Target language is Myanmar\n",
    "languages   = [SRC_LANGUAGE, TRG_LANGUAGE]\n",
    "\n",
    "for data in dataset:\n",
    "# english myanmar data\n",
    "    datasetENMY[data] = [{lang: row['translation'][lang] for lang in languages} for row in dataset[data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 18088\n",
      "validation : 1000\n",
      "test : 1019\n"
     ]
    }
   ],
   "source": [
    "# check the size for each dataset\n",
    "for data in datasetENMY:\n",
    "    print(f\"{data} : {len(datasetENMY[data])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetENMY['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[SRC_LANGUAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[TRG_LANGUAGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source language <\"ENG\"> tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language = 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'TimesOnline',\n",
       " 'reports',\n",
       " 'that',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'fell',\n",
       " 'flat',\n",
       " 'with',\n",
       " 'Jeffrey',\n",
       " 'Turner',\n",
       " ',',\n",
       " 'who',\n",
       " 'as',\n",
       " 'Chief',\n",
       " 'of',\n",
       " 'Police',\n",
       " 'in',\n",
       " 'Clayton',\n",
       " 'County',\n",
       " ',',\n",
       " 'Georgia',\n",
       " ',',\n",
       " 'put',\n",
       " 'Mr',\n",
       " 'Whitton',\n",
       " 'on',\n",
       " 'medical',\n",
       " 'leave',\n",
       " 'when',\n",
       " 'he',\n",
       " 'was',\n",
       " 'shot',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wrist',\n",
       " 'as',\n",
       " 'he',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'foil',\n",
       " 'a',\n",
       " 'robbery',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'summer',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[SRC_LANGUAGE]\n",
    "token_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target Language <\"MYR\"> tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code is updated version of this: https://gist.github.com/markdtw/e2a4e2ee7cef8ea6aed33bb47a97fba6\n",
    "Ye Kyaw Thu, LST, NECTEC, Thailand updated followings:\n",
    "-- added recursion limit\n",
    "-- changed P_unigram and P_bigram as module level global variable\n",
    "-- using binary ngram dictionary\n",
    "--  set N value of this: \"def __init__(self, datafile=None, unigram=True, N=102490):\"\n",
    "-- Last Updated: 5 Sept 2021\n",
    "\n",
    "# References:\n",
    "- Python implementation of Viterbi algorithm for word segmentation: \n",
    "- Updated version of this: https://gist.github.com/markdtw/e2a4e2ee7cef8ea6aed33bb47a97fba6\n",
    "- A clean-up of this: http://norvig.com/ngrams/ch14.pdf\n",
    "- For recursion limit: https://www.geeksforgeeks.org/python-handling-recursion-limit/\n",
    "- A. Viterbi, \"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,\" in IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260-269, April 1967, doi: 10.1109/TIT.1967.1054010.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import functools\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.setrecursionlimit(10**6)\n",
    "\n",
    "uni_dict_bin = './data/unigram-word.bin'\n",
    "bi_dict_bin = './data/bigram-word.bin'                \n",
    "\n",
    "def read_dict (fileDICT):\n",
    "    try:\n",
    "        with open(fileDICT, 'rb') as input_file:\n",
    "            dictionary = pickle.load(input_file)\n",
    "            input_file.close()\n",
    "    except FileNotFoundError:\n",
    "        print('Dictionary file', fileDICT, ' not found!')\n",
    "    return dictionary\n",
    "\n",
    "class ProbDist(dict):\n",
    "    ### Probability distribution estimated from unigram/bigram data\n",
    "    def __init__(self, datafile=None, unigram=True, N=102490):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=1024908267229):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=8199266137832):\n",
    "        #data = {}\n",
    "        data = read_dict(datafile)\n",
    "        for k, c in data.items():\n",
    "            self[k] = self.get(k, 0) + c\n",
    "\n",
    "        if unigram:\n",
    "            self.unknownprob = lambda k, N: 10 / (N*10**len(k))    # avoid unknown long word\n",
    "        else:\n",
    "            self.unknownprob = lambda k, N: 1 / N\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else:\n",
    "            return self.unknownprob(key, self.N)\n",
    "        \n",
    "\n",
    "P_unigram = ProbDist(uni_dict_bin, True)\n",
    "P_bigram = ProbDist(bi_dict_bin, False)\n",
    "\n",
    "\n",
    "def conditionalProb(word_curr, word_prev):\n",
    "    ### Conditional probability of current word given the previous word.\n",
    "    try:\n",
    "        return P_bigram[word_prev + ' ' + word_curr]/P_unigram[word_prev]\n",
    "    except KeyError:\n",
    "        return P_unigram(word_curr)\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=2**10)\n",
    "#maxlen=20\n",
    "def viterbi(text, prev='<S>', maxlen=20):\n",
    "    if not text:\n",
    "        return 0.0, []\n",
    "    \n",
    "    #print(\"text: \", text)\n",
    "    textlen = min(len(text), maxlen)\n",
    "    splits = [(text[:i + 1], text[i + 1:]) for i in range(textlen)]\n",
    "\n",
    "    candidates = []\n",
    "    #print(\"clear candidates!  candidates = []\")\n",
    "    for first_word, remain_word in splits:\n",
    "        #pdb.set_trace()\n",
    "        first_prob = math.log10(conditionalProb(first_word, prev))\n",
    "        #print(\"first_prob of condProb(\", first_word, \", \", prev, \"): \", first_prob )\n",
    "        remain_prob, remain_word = viterbi(remain_word, first_word)\n",
    "        #print(\"remain_prob: \", remain_prob, \", remain_word: \", remain_word)\n",
    "        candidates.append((first_prob + remain_prob, [first_word] + remain_word))\n",
    "        #print(\"first_prob: \", str(first_prob), \", remain_prob: \", remain_prob, \", [first_word]:\", [first_word], \", remain_word: \", remain_word)\n",
    "        #print(\"Candidates: \", candidates)\n",
    "        \n",
    "    #print(\"max(candidates): \" + str(max(candidates)))\n",
    "    #print(\"====================\")\n",
    "    return max(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    wordDelimiter= '|' # assign local variable delimiter\n",
    "\n",
    "    input = text[:]\n",
    "    # text = corpus['train'][0][TRG_LANGUAGE]\n",
    "    listString = viterbi(input.replace(\" \", \"\").strip()) # remove space between words and pass to viterbi()\n",
    "    # print(\"listString: \" + str(listString))\n",
    "    wordStr = wordDelimiter.join(listString[1])\n",
    "    wordClean1=wordStr.strip()\n",
    "    wordClean2=wordClean1.strip(wordDelimiter)    \n",
    "    wordClean2 = wordClean2.split('|')                \n",
    "    return wordClean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.',\n",
       " 'my': 'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[TRG_LANGUAGE] = my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ယခု',\n",
       " 'နွေရာသီ',\n",
       " 'အစောပိုင်း',\n",
       " 'ကာလ',\n",
       " '၌',\n",
       " 'လုယက်',\n",
       " 'မှု',\n",
       " 'တစ်',\n",
       " 'ခု',\n",
       " 'ကျူးလွန်',\n",
       " 'ရန်',\n",
       " 'ကြိုးစား',\n",
       " 'ခဲ့',\n",
       " 'သော',\n",
       " 'ကြောင့်',\n",
       " 'လက်',\n",
       " 'ကောက်',\n",
       " '၀',\n",
       " 'တ်',\n",
       " 'တွင်',\n",
       " 'သေနတ်ကျည်',\n",
       " 'မှန်',\n",
       " 'ခဲ့',\n",
       " 'သော',\n",
       " 'မ',\n",
       " 'စ်',\n",
       " 'စ',\n",
       " 'တာ',\n",
       " '၀ှစ်',\n",
       " 'တွန်',\n",
       " 'ကို',\n",
       " 'က',\n",
       " 'လေ',\n",
       " 'တွန်',\n",
       " 'ကောင်',\n",
       " 'တီ',\n",
       " '၊',\n",
       " 'ဂျော်ဂျီယာ',\n",
       " 'ပြည်နယ်',\n",
       " 'မှ',\n",
       " '၊',\n",
       " 'ရဲမှူးကြီး',\n",
       " 'ဂျက်',\n",
       " 'ဖရီ',\n",
       " 'တာ',\n",
       " 'နာ',\n",
       " 'မှ',\n",
       " 'ဆေး',\n",
       " 'ခွင့်',\n",
       " 'ပေး',\n",
       " 'တာ',\n",
       " 'နှင့်',\n",
       " 'ပတ်',\n",
       " 'သတ်',\n",
       " 'ပြီး',\n",
       " '၊',\n",
       " 'တိုင်းမ်စ်',\n",
       " 'အွန်လိုင်း',\n",
       " 'မှ',\n",
       " 'ဟာသ',\n",
       " 'ပြက်လုံး',\n",
       " 'တစ်',\n",
       " 'ခု',\n",
       " 'ကို',\n",
       " 'မှတ်တမ်းတင်',\n",
       " 'ရေး',\n",
       " 'သား',\n",
       " 'ခဲ့',\n",
       " 'သည်',\n",
       " '။']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "corpus = copy.deepcopy(datasetENMY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# make sure the tockens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be 'train' or 'val' or 'test' \n",
    "def yield_tokens(data, language):\n",
    "    # language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE:1}\n",
    "    \n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "        # either first or second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(corpus['train'], ln), \n",
    "                                                    min_freq = 2,   # if not, everything will be treated as UNK\n",
    "                                                    specials = special_symbols,\n",
    "                                                    special_first = True) # indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(vocab_transform, open('./data/vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = pickle.load(open('./data/vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['my']['သူ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence \n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# # helper function to club together sequential operations\n",
    "# def sequential_transforms(*transforms):\n",
    "#     def func(txt_input):\n",
    "#         for transform in transforms:\n",
    "#             txt_input = transform(txt_input)\n",
    "#         return txt_input\n",
    "#     return func\n",
    "\n",
    "# # function to add BOS/EOS and crete tensor for input sequence indices\n",
    "# def tensor_transform (token_ids):\n",
    "#     return torch.cat((torch.tensor([SOS_IDX]),\n",
    "#                       torch.tensor(token_ids),\n",
    "#                       torch.tensor([EOS_IDX])))\n",
    "    \n",
    "# # src and trg language text transforms to convert raw strings into tensors indices\n",
    "# text_transform = {}\n",
    "# for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "#     text_transform[ln] = sequential_transforms(token_transform[ln], # tokenization\n",
    "#                                                voc[ln], # Numericalization\n",
    "#                                                tensor_transform # Add BOS/ EOS and create tesor\n",
    "#                                                )\n",
    "    \n",
    "# # function to collate data samples into batch tensors\n",
    "# def collate_batch(batch):\n",
    "#     src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    \n",
    "#     for src_sample, trg_sample in batch:\n",
    "#         processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "#         src_batch.append(processed_text)\n",
    "#         trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "#         src_len_batch.append(processed_text.size(0))\n",
    "#         # to get the size of the processed text along the first dimension. \n",
    "#         # In the context of PyTorch or similar tensor libraries, this typically corresponds to the length of the text.\n",
    "        \n",
    "#     src_batch = pad_sequence(src_batch, padding_value = PAD_IDX) # pads the input sequences with PAD_IDX to make them of equal length.\n",
    "#     trg_batch = pad_sequence(trg_batch, padding_value = PAD_IDX)\n",
    "    \n",
    "#     return src_batch, torch.tensor(src_len_batch, dtype = torch.int64), trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "\n",
    "    for lang_data in batch:\n",
    "        # Assuming that each language is a key in the dictionary\n",
    "        for lang, tokens in lang_data.items():\n",
    "            processed_text = text_transform[lang](tokens)\n",
    "            \n",
    "            # For source language, append to src_batch and record length\n",
    "            if lang == SRC_LANGUAGE:\n",
    "                src_batch.append(processed_text)\n",
    "                src_len_batch.append(processed_text.size(0))\n",
    "            \n",
    "            # For target language, append to trg_batch\n",
    "            elif lang == TRG_LANGUAGE:\n",
    "                trg_batch.append(processed_text)\n",
    "\n",
    "    # Pad sequences\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(corpus['train'], batch_size = batch_size, shuffle = True , collate_fn = collate_batch)\n",
    "valid_loader = DataLoader(corpus['validation']  , batch_size = batch_size, shuffle = False, collate_fn = collate_batch)\n",
    "test_loader  = DataLoader(corpus['test'] , batch_size = batch_size, shuffle = False, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, my in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18088"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape: torch.Size([62, 64])\n",
      "Myanmar shpae torch.Size([90, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape:\", en.shape)\n",
    "print(\"Myanmar shpae\", my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2],\n",
       "        [5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482,\n",
       "         5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482,\n",
       "         5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482,\n",
       "         5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482,\n",
       "         5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482, 5482,\n",
       "         5482, 5482, 5482, 5482],\n",
       "        [   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device  = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch_size]\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so that it's the same shape as attention\n",
    "        #mask: [batch_size, src len] #(0, 0, 0, 0, 0, 1, 1)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src: [src len, batch_size]\n",
    "        #trg: [trg len, batch_size]\n",
    "        \n",
    "        #initialize something\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len    = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs    = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #send our src text into encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        #encoder_outputs refer to all hidden states (last layer)\n",
    "        #hidden refer to the last hidden state (of each layer, of each direction)\n",
    "        \n",
    "        input_ = trg[0, :]\n",
    "        \n",
    "        mask   = self.create_mask(src) #(0, 0, 0, 0, 0, 1, 1) \n",
    "        #masked_filled default sets to preset value\n",
    "        #when the cell value is True, i.e. 1\n",
    "        \n",
    "        #for each of the input of the trg text\n",
    "        for t in range(1, trg_len):\n",
    "            #send them to the decoder\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output: [batch_size, output_dim] ==> predictions\n",
    "            #hidden: [batch_size, hid_dim]\n",
    "            #attention: [batch_size, src len]\n",
    "            \n",
    "            #append the output to a list\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1          = output.argmax(1)  #autoregressive\n",
    "            \n",
    "            input_ = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional = True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward (self, src, src_len):\n",
    "        \n",
    "        # embedding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # packed\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted = False)\n",
    "        \n",
    "        # rnn\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        # unpacked\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        # -1, -2 hidden state\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        # output: [src len, batch size, hid dim * 2]\n",
    "        # hidden: [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim, hid_dim)         # for decoder input_\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) # for encoder_outputs\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden : [batch_size, hid_dim] ==> first hidden is basically the last hidden of the encoder\n",
    "        # encoder_output: [src_len, batch_size, hid_dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len    = encoder_outputs.shape[0]\n",
    "        \n",
    "        # repeat the hidden src len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden : [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # permute the encoder_outputs just so that you can perform multiplication / addition\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs : [batch_size, src_len, hid_dim * 2]\n",
    "        \n",
    "        # add\n",
    "        energy = self.v(torch.tanh(self.W(hidden) + self.U(encoder_outputs))).squeeze(2)\n",
    "        # [batch_size, src_len, 1] ==> [batch_size, src_len]\n",
    "        \n",
    "        # mask\n",
    "        energy = energy.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(energy, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicative Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention  = attention\n",
    "        self.embedding  = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn        = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc         = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        #input: [batch_size]\n",
    "        #hidden: [batch_size, hid_dim]\n",
    "        #encoder_ouputs: [src len, batch_size, hid_dim * 2]\n",
    "        #mask: [batch_size, src len]\n",
    "                \n",
    "        #embed our input\n",
    "        input    = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        #calculate the attention\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch_size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_ouputs: [batch_size, src len, hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted: [batch_size, 1, hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted: [1, batch_size, hid_dim * 2]\n",
    "        \n",
    "        #send the input to decoder rnn\n",
    "            #concatenate (embed, weighted encoder_outputs)\n",
    "            #[1, batch_size, emb_dim]; [1, batch_size, hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input: [1, batch_size, emb_dim + hid_dim * 2]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "            \n",
    "        #send the output of the decoder rnn to fc layer to predict the word\n",
    "            #prediction = fc(concatenate (output, weighted, embed))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((embedded, output, weighted), dim = 1))\n",
    "        #prediction: [batch_size, output_dim]\n",
    "            \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(16912, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(8029, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=8029, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additive Attention\n",
    "\n",
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = AdditiveAttention(hid_dim)\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "addmodel = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "addmodel.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329472\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "2055424\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "14387968\n",
      "  8029\n",
      "______\n",
      "27214685\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(addmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(addmodel.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg    = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(valid_loader)\n",
    "test_loader_length  = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time calculation function\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 261m 56s\n",
      "\tTrain Loss: 6.967 | Train PPL: 1061.397\n",
      "\t Val. Loss: 6.411 |  Val. PPL: 608.562\n",
      "Epoch: 02 | Time: 82m 30s\n",
      "\tTrain Loss: 5.684 | Train PPL: 294.165\n",
      "\t Val. Loss: 6.203 |  Val. PPL: 494.089\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 2\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'./models/addmodel.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(addmodel, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(addmodel, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(addmodel.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
