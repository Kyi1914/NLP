{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3 - Neural Machine Translation (Myanmar to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm # progress bar\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 18088\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1019\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "    num_rows: 18088\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNT.URLID': '87564',\n",
       " 'SNT.URLID.SNTID': '13',\n",
       " 'url': 'http://en.wikinews.org/wiki/Data_for_3_million_UK_driving_candidates_lost',\n",
       " 'translation': {'bg': 'অক্টোবরে এইচএম রেভিনিউ ২৫ মিলিয়ন লোকের তথ্য হারিয়ে ফেলার পরে এটিই হল ইউকে-তে প্রথম এত বড় তথ্যের ক্ষতি।',\n",
       "  'en': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October.',\n",
       "  'en_tok': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October .',\n",
       "  'fil': 'Ito ang unang malakihang pagkawala ng data sa UK dahil ang impormasyon sa 25 milyong tao ay nawalan ng HM Revenue noong Oktubre.',\n",
       "  'hi': 'यह ब्रिटेन में डेटा का पहला बड़ा नुकसान है क्योंकि अक्टूबर में HM रेवेन्यू द्वारा 25 मिलियन लोगों की जानकारी गुम हो गई थी।',\n",
       "  'id': 'Ini adalah kehilangan data yang besar pertama di UK sejak hilangnya informasi tentang 25 juta orang oleh HM Revenue di bulan Oktober.',\n",
       "  'ja': 'これは、10月に歳入関税庁が2500万人分の情報を失って以来初めてのイギリスでの大きなデータ紛失である。',\n",
       "  'khm': 'នេះជាការបាត់បង់ទិន្នន័យដ៏ធំំលើកទីមួយក្នុងចក្រភពអង់គ្លេសបន្ទាប់ពីបានបាត់បង់ព័ត៌មាន25លាននាក់នៅទីភ្នាក់ងារចំណូលគយនៅខែតុលា។',\n",
       "  'lo': 'ນີ້ແມ່ນການສູນເສຍຄັ້ງສຳຄັນຂອງຂໍ້ມູນໃນ ສະຫະລາຊະອານາຈັກ ຕັ້ງແຕ່ຂໍ້ມູນຂອງ 25 ລ້ານຄົນ ໄດ້ສູນຫາຍໄປໂດຍ HM ເຮເວີນູ ໃນເດືອນຕຸລາ.',\n",
       "  'ms': 'Ini adalah kehilangan data terbesar di UK semenjak maklumat 25 juta orang dihilangkan oleh HM Revenue pada Oktober.',\n",
       "  'my': 'အဆိုပါကိစ္စ သည် အောက်တိုဘာလ တွင် လူ ၂၅ သန်း ၏ သတင်းအချက်အလက်များ အခွန်ဝန်ကြီးဌာန ကြောင့် ဆုံးရှုံးခဲ့ ကတည်းက ဗြိတိန်နိုင်ငံ တွင် ပထမဦးဆုံးသော အဓိကကျသည့် အချက်အလက်များ ၏ ပျောက်ဆုံးမှု ဖြစ်သည် ။',\n",
       "  'th': 'นับเป็นการสูญหายของข้อมูลครั้งใหญ่ครั้งแรกในUK ตั้งแต่ข้อมูลของประชาชน25 ล้านคนสูญหายโดยHMของสรรพากรเมื่อเดือนตุลาคม',\n",
       "  'vi': 'Đây là vụ mất dữ liệu lớn đầu tiên tại Anh kể từ khi Cục Thuế làm mất dữ liệu của 25 triệu người vào tháng Mười.',\n",
       "  'zh': '这是自去年10月英国税务机构丢失2500万人信息以来，英国首次出现重大数据丢失。'}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bg': 'গতকাল ১৭৩০ ইউটিসি-তে হাউস অফ্\\u200c কমন্\\u200cস্\\u200c-এ ইউকে-র পরিবহন সচিব রুথ কেলি এই তথ্যগুলি দিয়েছেন।',\n",
       " 'en': 'Details were given by the UK Transport Secretary, Ruth Kelly, in the House of Commons at 1730 UTC yesterday.',\n",
       " 'en_tok': 'Details were given by the UK Transport Secretary , Ruth Kelly , in the House of Commons at 1730 UTC yesterday .',\n",
       " 'fil': 'Ang mga detalye ay ibinigay ng UK transport Secretary, na si Ruth Kelly, sa House of Commons sa ika-17:30 UTC kahapon.',\n",
       " 'hi': 'कल ब्रिटेन के परिवहन सचिव रूथ केली द्वारा 1730 UTC पर हाउस ऑफ़ कॉमन्स में विवरण दिए गए।',\n",
       " 'id': 'Detil diberikan oleh Sekretaris Kementerian Transportasi UK, Ruth Kelly, di Dewan Perwakilan Rakyat kemarin 17:30 UTC.',\n",
       " 'ja': '詳細は昨日UTC17時30分、英国議会でイギリスのルス・ケリー運輸大臣によって伝えられた。',\n",
       " 'khm': 'ព័ត៌មានលំអិតត្រូវបានផ្តល់ដោយរដ្ឋមន្ត្រីដឹកជញ្ចូន លោករ៉ូថ ខេលលី នៅក្នុងសភានៅម៉ោង1730ម្សិលមិញ។',\n",
       " 'lo': 'ຂໍ້ມູນໄດ້ຖືກສະໜອງໂດຍ ເລຂາທິການຂົນສົ່ງ ສະຫະລາຊະອານາຈັກ ຣູດ ເຄລີ່ ໃນສະພາຕໍ່າ ທີ່ 1730 UTC ມື້ວານນີ້.',\n",
       " 'ms': 'Butiran telah diberikan oleh Setiausaha Pengangkutan UK, Ruth Kelly, di Dewan Rakyat pada 1730 UTC semalam.',\n",
       " 'my': 'အသေးစိတ်များ ကို မနေ့က ၁၇၃၀ ယူတီစီ ၌ အောက်လွှတ်တော် ရှိ ဗြိတိန်နိုင်ငံ ပို့ဆောင်ရေး အတွင်းရေးမှူး ရုသ်ကယ်လီ က ပေးခဲ့သည် ။',\n",
       " 'th': 'รายละเอียดถูกเปิดเผยโดยเลขาธิการกระทรวงคมนาคมของUK นางRuth Kelly ในสภาสามัญชน เมื่อเวลา17:30 UTC',\n",
       " 'vi': 'Thông tin chi tiết được cung cấp bởi Bộ trưởng Giao thông Anh, Ruth Kelly, tại Hạ nghị viện ngày hôm qua lúc 17:30 theo Giờ Phối hợp Quốc tế.',\n",
       " 'zh': '英国交通大臣露丝·凯利昨天于英国时间17：30分在下议院公布了详细情况。'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Details were given by the UK Transport Secretary, Ruth Kelly, in the House of Commons at 1730 UTC yesterday.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'အသေးစိတ်များ ကို မနေ့က ၁၇၃၀ ယူတီစီ ၌ အောက်လွှတ်တော် ရှိ ဗြိတိန်နိုင်ငံ ပို့ဆောင်ရေး အတွင်းရေးမှူး ရုသ်ကယ်လီ က ပေးခဲ့သည် ။'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][11]['translation']['my']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset that will contain only Myanmar language as a target language and English language as a source language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetENMY = {}\n",
    "\n",
    "# Define source and target languages\n",
    "SRC_LANGUAGE = 'en'  # Source language is English\n",
    "TRG_LANGUAGE = 'my'  # Target language is Myanmar\n",
    "languages   = [SRC_LANGUAGE, TRG_LANGUAGE]\n",
    "\n",
    "for data in dataset:\n",
    "# english myanmar data\n",
    "    datasetENMY[data] = [{lang: row['translation'][lang] for lang in languages} for row in dataset[data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 18088\n",
      "validation : 1000\n",
      "test : 1019\n"
     ]
    }
   ],
   "source": [
    "# check the size for each dataset\n",
    "for data in datasetENMY:\n",
    "    print(f\"{data} : {len(datasetENMY[data])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetENMY['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[SRC_LANGUAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[TRG_LANGUAGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source language <\"ENG\"> tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language = 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'TimesOnline',\n",
       " 'reports',\n",
       " 'that',\n",
       " 'the',\n",
       " 'joke',\n",
       " 'fell',\n",
       " 'flat',\n",
       " 'with',\n",
       " 'Jeffrey',\n",
       " 'Turner',\n",
       " ',',\n",
       " 'who',\n",
       " 'as',\n",
       " 'Chief',\n",
       " 'of',\n",
       " 'Police',\n",
       " 'in',\n",
       " 'Clayton',\n",
       " 'County',\n",
       " ',',\n",
       " 'Georgia',\n",
       " ',',\n",
       " 'put',\n",
       " 'Mr',\n",
       " 'Whitton',\n",
       " 'on',\n",
       " 'medical',\n",
       " 'leave',\n",
       " 'when',\n",
       " 'he',\n",
       " 'was',\n",
       " 'shot',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wrist',\n",
       " 'as',\n",
       " 'he',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'foil',\n",
       " 'a',\n",
       " 'robbery',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'summer',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[SRC_LANGUAGE]\n",
    "token_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Target Language <\"MYR\"> tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code is updated version of this: https://gist.github.com/markdtw/e2a4e2ee7cef8ea6aed33bb47a97fba6\n",
    "Ye Kyaw Thu, LST, NECTEC, Thailand updated followings:\n",
    "-- added recursion limit\n",
    "-- changed P_unigram and P_bigram as module level global variable\n",
    "-- using binary ngram dictionary\n",
    "--  set N value of this: \"def __init__(self, datafile=None, unigram=True, N=102490):\"\n",
    "-- Last Updated: 5 Sept 2021\n",
    "\n",
    "# References:\n",
    "- Python implementation of Viterbi algorithm for word segmentation: \n",
    "- Updated version of this: https://gist.github.com/markdtw/e2a4e2ee7cef8ea6aed33bb47a97fba6\n",
    "- A clean-up of this: http://norvig.com/ngrams/ch14.pdf\n",
    "- For recursion limit: https://www.geeksforgeeks.org/python-handling-recursion-limit/\n",
    "- A. Viterbi, \"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,\" in IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260-269, April 1967, doi: 10.1109/TIT.1967.1054010.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import functools\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.setrecursionlimit(10**6)\n",
    "\n",
    "uni_dict_bin = './data/unigram-word.bin'\n",
    "bi_dict_bin = './data/bigram-word.bin'                \n",
    "\n",
    "def read_dict (fileDICT):\n",
    "    try:\n",
    "        with open(fileDICT, 'rb') as input_file:\n",
    "            dictionary = pickle.load(input_file)\n",
    "            input_file.close()\n",
    "    except FileNotFoundError:\n",
    "        print('Dictionary file', fileDICT, ' not found!')\n",
    "    return dictionary\n",
    "\n",
    "class ProbDist(dict):\n",
    "    ### Probability distribution estimated from unigram/bigram data\n",
    "    def __init__(self, datafile=None, unigram=True, N=102490):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=1024908267229):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=8199266137832):\n",
    "        #data = {}\n",
    "        data = read_dict(datafile)\n",
    "        for k, c in data.items():\n",
    "            self[k] = self.get(k, 0) + c\n",
    "\n",
    "        if unigram:\n",
    "            self.unknownprob = lambda k, N: 10 / (N*10**len(k))    # avoid unknown long word\n",
    "        else:\n",
    "            self.unknownprob = lambda k, N: 1 / N\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else:\n",
    "            return self.unknownprob(key, self.N)\n",
    "        \n",
    "\n",
    "P_unigram = ProbDist(uni_dict_bin, True)\n",
    "P_bigram = ProbDist(bi_dict_bin, False)\n",
    "\n",
    "\n",
    "def conditionalProb(word_curr, word_prev):\n",
    "    ### Conditional probability of current word given the previous word.\n",
    "    try:\n",
    "        return P_bigram[word_prev + ' ' + word_curr]/P_unigram[word_prev]\n",
    "    except KeyError:\n",
    "        return P_unigram(word_curr)\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=2**10)\n",
    "#maxlen=20\n",
    "def viterbi(text, prev='<S>', maxlen=20):\n",
    "    if not text:\n",
    "        return 0.0, []\n",
    "    \n",
    "    #print(\"text: \", text)\n",
    "    textlen = min(len(text), maxlen)\n",
    "    splits = [(text[:i + 1], text[i + 1:]) for i in range(textlen)]\n",
    "\n",
    "    candidates = []\n",
    "    #print(\"clear candidates!  candidates = []\")\n",
    "    for first_word, remain_word in splits:\n",
    "        #pdb.set_trace()\n",
    "        first_prob = math.log10(conditionalProb(first_word, prev))\n",
    "        #print(\"first_prob of condProb(\", first_word, \", \", prev, \"): \", first_prob )\n",
    "        remain_prob, remain_word = viterbi(remain_word, first_word)\n",
    "        #print(\"remain_prob: \", remain_prob, \", remain_word: \", remain_word)\n",
    "        candidates.append((first_prob + remain_prob, [first_word] + remain_word))\n",
    "        #print(\"first_prob: \", str(first_prob), \", remain_prob: \", remain_prob, \", [first_word]:\", [first_word], \", remain_word: \", remain_word)\n",
    "        #print(\"Candidates: \", candidates)\n",
    "        \n",
    "    #print(\"max(candidates): \" + str(max(candidates)))\n",
    "    #print(\"====================\")\n",
    "    return max(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    wordDelimiter= '|' # assign local variable delimiter\n",
    "\n",
    "    input = text[:]\n",
    "    # text = corpus['train'][0][TRG_LANGUAGE]\n",
    "    listString = viterbi(input.replace(\" \", \"\").strip()) # remove space between words and pass to viterbi()\n",
    "    # print(\"listString: \" + str(listString))\n",
    "    wordStr = wordDelimiter.join(listString[1])\n",
    "    wordClean1=wordStr.strip()\n",
    "    wordClean2=wordClean1.strip(wordDelimiter)    \n",
    "    wordClean2 = wordClean2.split('|')                \n",
    "    return wordClean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.',\n",
       " 'my': 'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[TRG_LANGUAGE] = my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ယခု',\n",
       " 'နွေရာသီ',\n",
       " 'အစောပိုင်း',\n",
       " 'ကာလ',\n",
       " '၌',\n",
       " 'လုယက်',\n",
       " 'မှု',\n",
       " 'တစ်',\n",
       " 'ခု',\n",
       " 'ကျူးလွန်',\n",
       " 'ရန်',\n",
       " 'ကြိုးစား',\n",
       " 'ခဲ့',\n",
       " 'သော',\n",
       " 'ကြောင့်',\n",
       " 'လက်',\n",
       " 'ကောက်',\n",
       " '၀',\n",
       " 'တ်',\n",
       " 'တွင်',\n",
       " 'သေနတ်ကျည်',\n",
       " 'မှန်',\n",
       " 'ခဲ့',\n",
       " 'သော',\n",
       " 'မ',\n",
       " 'စ်',\n",
       " 'စ',\n",
       " 'တာ',\n",
       " '၀ှစ်',\n",
       " 'တွန်',\n",
       " 'ကို',\n",
       " 'က',\n",
       " 'လေ',\n",
       " 'တွန်',\n",
       " 'ကောင်',\n",
       " 'တီ',\n",
       " '၊',\n",
       " 'ဂျော်ဂျီယာ',\n",
       " 'ပြည်နယ်',\n",
       " 'မှ',\n",
       " '၊',\n",
       " 'ရဲမှူးကြီး',\n",
       " 'ဂျက်',\n",
       " 'ဖရီ',\n",
       " 'တာ',\n",
       " 'နာ',\n",
       " 'မှ',\n",
       " 'ဆေး',\n",
       " 'ခွင့်',\n",
       " 'ပေး',\n",
       " 'တာ',\n",
       " 'နှင့်',\n",
       " 'ပတ်',\n",
       " 'သတ်',\n",
       " 'ပြီး',\n",
       " '၊',\n",
       " 'တိုင်းမ်စ်',\n",
       " 'အွန်လိုင်း',\n",
       " 'မှ',\n",
       " 'ဟာသ',\n",
       " 'ပြက်လုံး',\n",
       " 'တစ်',\n",
       " 'ခု',\n",
       " 'ကို',\n",
       " 'မှတ်တမ်းတင်',\n",
       " 'ရေး',\n",
       " 'သား',\n",
       " 'ခဲ့',\n",
       " 'သည်',\n",
       " '။']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "corpus = copy.deepcopy(datasetENMY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# make sure the tockens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be 'train' or 'val' or 'test' \n",
    "def yield_tokens(data, language):\n",
    "    # language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE:1}\n",
    "    \n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "        # either first or second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(corpus['train'], ln), \n",
    "                                                    min_freq = 2,   # if not, everything will be treated as UNK\n",
    "                                                    specials = special_symbols,\n",
    "                                                    special_first = True) # indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
