{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2 - Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm # progress bar\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 # to generate the same results\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('KyiThinNu/FairyTales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 102542\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 48274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 41472\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# access the text data inside the dataset.\n",
    "# print(dataset['which dataset'][which row]['which features'])\n",
    "print(dataset['train'][223]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102542, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].shape) # (row, col) (36718 rows of text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cbff64fdf7456bb241259d3fac26f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/102542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54088a11a7a3488cbd04808fd25d7cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48274 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb2ac02e3804df19c454824d209c767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# create a function\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# def tokenize_data(example, tokenizer):\n",
    "#     tokens = tokenizer(example['text'])\n",
    "#     return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens'],\n",
       "        num_rows: 102542\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens'],\n",
       "        num_rows: 48274\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens'],\n",
       "        num_rows: 41472\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][223]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator (tokenized_dataset['train']['tokens'], min_freq = 3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12950\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', ',', 'the', 'and', '.', 'of', 'to', 'a', 'he']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset: # get >> example = Chaky loves eating at AIT\n",
    "        if example['tokens']:\n",
    "            \n",
    "            # add '<eos>' at the end of each sentence, (example) in this case, inside dataset \n",
    "            # ['Chaky', 'loves', 'eating', 'at', 'AIT', '<eos>']\n",
    "            example['tokens'].append('<eos>') \n",
    "            \n",
    "            # apply numericalization\n",
    "            tokens = [vocab[token] for token in example['tokens']] # [6,2,3,5,1]\n",
    "            data.extend(tokens)\n",
    "    \n",
    "    # convert data type to torch for embedding        \n",
    "    data = torch.LongTensor(data)\n",
    "    \n",
    "    num_batches = data.shape[0] // batch_size # // is integer division 4\n",
    "    \n",
    "    # just to make sure all batches are even\n",
    "    data = data[:num_batches * batch_size] # to update the data size that is divisable by batch_size, consider odd number for data.shape[0]\n",
    "    print(data.shape)\n",
    "    \n",
    "    data = data.view(batch_size, num_batches) # (3,4) # view vs. reshape (whether data is contiguous)\n",
    "    print(data.shape)\n",
    "    \n",
    "    return data # [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1201920])\n",
      "torch.Size([128, 9390])\n",
      "torch.Size([545408])\n",
      "torch.Size([128, 4261])\n",
      "torch.Size([485888])\n",
      "torch.Size([128, 3796])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 9390])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape # each train has total 128 batch and each batch contain 9390 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers = num_layers, \n",
    "                                  dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb   = 0.1 \n",
    "        init_range_other = 1 / math.sqrt(self.hid_dim) \n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb) \n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(\n",
    "                -init_range_other, init_range_other) # We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, self.hid_dim).uniform_(\n",
    "                -init_range_other, init_range_other) # Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden # return from LSTM that is tuple that contains hidden and cell values\n",
    "        hidden       = hidden.detach() # not to be used for gradient computation\n",
    "        cell         = cell.detach()\n",
    "        return hidden, cell\n",
    "    \n",
    "    def forward(self, src, hidden):\n",
    "        \n",
    "        # src: [batch_size, seq_len]\n",
    "        \n",
    "        embedding      = self.dropout(self.embedding(src)) # src = harry potter is \n",
    "        # embedding: [batch-size, seq_len, emb_dim]\n",
    "        \n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        # output: [batch_size, seq_len, emb_dim]\n",
    "        #hidden: [num_layers * direction, seq_len, hid_dim]\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        prediction = self.fc(output)\n",
    "        # predcition: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024 # 400 in the paper\n",
    "hid_dim = 1024 # 1150 in the paper\n",
    "num_layers = 2 # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model has 43,328,150 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'the model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch (data, seq_len, idx):\n",
    "    # data #[batch_size, bunch of tokens]\n",
    "    src    = data[:, idx  : idx+seq_len]\n",
    "    target = data[:, idx+1: idx+seq_len+1] # target simply is ahead of src by 1\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch_size, seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data        = data[:, :num_batches - (num_batches - 1) % seq_len] # we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    # reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches-1, seq_len), desc = 'Training: ', leave = False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) # src, target: [batch_size, seq_len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size  = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)      \n",
    "        \n",
    "        # need to reshape because criterion expects pred to be 2d and target to be 1d \n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target     = target.reshape(-1)\n",
    "        loss       = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip) # limit the exploding gradient (0.25)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, data, criterion, batch_size, seq_len, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.eval() # evaluation\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden      = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size  = src.shape[0]\n",
    "            \n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction         = prediction.reshape (batch_size * seq_len, -1)\n",
    "            target             = target.reshape(-1)\n",
    "            \n",
    "            loss              = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 # decoding length\n",
    "clip = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau (optimizer, factor = 0.5, patience = 0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate (model, valid_data, criterion, batch_size, seq_len, device)\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "        torch.save(model.state_dict(), 'fairy-tale-lstm.pt') # save the model\n",
    "        \n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data for later usage of model\n",
    "Data = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_dim' : emb_dim,\n",
    "    'hid_dim' : hid_dim,\n",
    "    'num_layers' : num_layers,\n",
    "    'dropout_rate' : dropout_rate,\n",
    "    # lr = 1e-3\n",
    "    'tokenizer' : tokenizer,\n",
    "    'vocab' : vocab\n",
    "}\n",
    "pickle.dump(Data, open('./app/model/Data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: 102.359\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt', map_location = device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "once upon a time , and\n",
      "\n",
      "0.7\n",
      "once upon a time of\n",
      "\n",
      "0.75\n",
      "once upon a time of\n",
      "\n",
      "0.8\n",
      "once upon a time of red\n",
      "\n",
      "1.0\n",
      "once upon a time forget .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'once upon a time'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tale', 'forget', 'dschu', ',']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
