{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1 : Search Engine (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.25.2', '2.1.0', '3.7.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset, I use the rural.txt from NLTK.net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\"apple banana grape orange fruit\", \"banana apple orange grape fruit\", \"banana fruit orange grape apple\",\n",
    "        #   \"dog cat mouse rabbit animal\", \"cat mouse rabbit animal dog\", \"cat dog animal rabbit mouse\"]\n",
    "\n",
    "# read the nltk dataset = rural\n",
    "txt_file = './abc/rural.txt'\n",
    "\n",
    "with open(txt_file, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "# Split the dataset into paragraphs based on double line breaks : to get one paragraph in a list item ['paragraph1', 'paragraph']\n",
    "paragraphs = [paragraph.strip() for paragraph in text.split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PM denies knowledge of AWB kickbacks\\nThe Prime Minister has denied he knew AWB was paying kickbacks to Iraq despite writing to the wheat exporter asking to be kept fully informed on Iraq wheat sales.\\nLetters from John Howard and Deputy Prime Minister Mark Vaile to AWB have been released by the Cole inquiry into the oil for food program.\\nIn one of the letters Mr Howard asks AWB managing director Andrew Lindberg to remain in close contact with the Government on Iraq wheat sales.\\nThe Opposition\\'s Gavan O\\'Connor says the letter was sent in 2002, the same time AWB was paying kickbacks to Iraq though a Jordanian trucking company.\\nHe says the Government can longer wipe its hands of the illicit payments, which totalled $290 million.\\n\"The responsibility for this must lay may squarely at the feet of Coalition ministers in trade, agriculture and the Prime Minister,\" he said.\\nBut the Prime Minister says letters show he was inquiring about the future of wheat sales in Iraq and do not prove the Government knew of the payments.\\n\"It would have been astonishing in 2002 if as Prime Minister I hadn\\'t done anything I possibly could to preserve Australia\\'s very valuable wheat market,\" he said.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(paragraphs)):\n",
    "    # Replace newline characters with spaces\n",
    "    paragraphs[i] = paragraphs[i].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PM denies knowledge of AWB kickbacks The Prime Minister has denied he knew AWB was paying kickbacks to Iraq despite writing to the wheat exporter asking to be kept fully informed on Iraq wheat sales. Letters from John Howard and Deputy Prime Minister Mark Vaile to AWB have been released by the Cole inquiry into the oil for food program. In one of the letters Mr Howard asks AWB managing director Andrew Lindberg to remain in close contact with the Government on Iraq wheat sales. The Opposition\\'s Gavan O\\'Connor says the letter was sent in 2002, the same time AWB was paying kickbacks to Iraq though a Jordanian trucking company. He says the Government can longer wipe its hands of the illicit payments, which totalled $290 million. \"The responsibility for this must lay may squarely at the feet of Coalition ministers in trade, agriculture and the Prime Minister,\" he said. But the Prime Minister says letters show he was inquiring about the future of wheat sales in Iraq and do not prove the Government knew of the payments. \"It would have been astonishing in 2002 if as Prime Minister I hadn\\'t done anything I possibly could to preserve Australia\\'s very valuable wheat market,\" he said.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. tokenization\n",
    "corpus = [sent.split(\" \") for sent in paragraphs]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the corpus only accept 100 documents\n",
    "corpus = corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the total word count in my corpus\n",
    "wc = 0\n",
    "for i in range(len(corpus)):\n",
    "    wc += len(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique word\n",
    "\n",
    "# list comprehension for getting words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# getting unique word and store as a list\n",
    "vocab = list(set(flatten(corpus)))\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <UNK> to a dictionary vocab\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization: assign index to each word\n",
    "word2index = {w:idx for idx, w in enumerate(vocab)}\n",
    "# word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2word\n",
    "index2word = {k:v for v,k in word2index.items()}\n",
    "# index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    # define a list for storing [center,outside] pair\n",
    "    skipgrams = []\n",
    "\n",
    "    # loop each word sequence\n",
    "    for sent in corpus:\n",
    "        \n",
    "        for i in range(2, len(sent)-2):\n",
    "            \n",
    "            # assign center word\n",
    "            center_word = word2index[sent[i]]\n",
    "            \n",
    "            # assign outside word=4 (ws = 2)\n",
    "            outside_word = [word2index[sent[i-2]], word2index[sent[i-1]], word2index[sent[i+1]], word2index[sent[i+2]]]\n",
    "            \n",
    "            # for each of these two outside words, we gonna pair (center,outside) and append to a list\n",
    "            for each_outside in outside_word:\n",
    "                skipgrams.append([center_word, each_outside])\n",
    "                \n",
    "    # randomly select 2 pair among the data\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace = False)\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skipgrams[i][0]]) # center_word\n",
    "        random_labels.append([skipgrams[i][1]]) # outside_word\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 895],\n",
       "        [ 993],\n",
       "        [3121],\n",
       "        [ 470]]),\n",
       " array([[1261],\n",
       "        [1342],\n",
       "        [ 625],\n",
       "        [2466]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "input_batch, label_batch = random_batch(4, corpus)\n",
    "input_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram (nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        \n",
    "        # embedding for all: result as a vector for each \n",
    "        center_embed  = self.embedding_center(center_word)   # (batch_size, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside(outside_word) # (batch_size, 1, emb_size)\n",
    "        all_embed     = self.embedding_outside(all_vocabs)   # (batch_size, voc_size, emb_size)\n",
    "        \n",
    "        # write the equation\n",
    "        top_term = torch.exp(outside_embed.bmm(center_embed.transpose(1,2)).squeeze(2))\n",
    "        # (batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (bacth_size, 1)\n",
    "        \n",
    "        lower_term = all_embed.bmm(center_embed.transpose(1,2)).squeeze(2)\n",
    "        # (batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)\n",
    "        # (batch_size, 1)\n",
    "        \n",
    "        loss = - torch.mean(torch.log(top_term / lower_term_sum))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test the skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_size = len(vocab)\n",
    "# emb_size = 2\n",
    "# model = Skipgram(voc_size, emb_size)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# x,y = random_batch(batch_size, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.LongTensor(x)\n",
    "# label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare for all_vocabs\n",
    "# def prepare_sequence (seq, word2index):\n",
    "#     idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "#     return torch.LongTensor(idxs)\n",
    "\n",
    "# all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab)) #[batch_size, voc_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab)) #[batch_size, voc_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size       = len(vocab) # total vocab size\n",
    "batch_size     = 2 # mini-batch size\n",
    "embedding_size = 2\n",
    "\n",
    "skipgram_model = Skipgram(voc_size, embedding_size)\n",
    "\n",
    "skipgram_optimizer = optim.Adam(skipgram_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for all_vocabs\n",
    "def prepare_sequence (seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab)) #[batch_size, voc_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ..., 4164, 4165, 4166],\n",
      "        [   0,    1,    2,  ..., 4164, 4165, 4166]])\n",
      "torch.Size([2, 4167])\n"
     ]
    }
   ],
   "source": [
    "print(all_vocabs)\n",
    "print(all_vocabs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for recording the training time for each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time # get the total taken timestamp\n",
    "    elapsed_mins = int(elapsed_time / 60) # get the min\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) # get the sec\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "import time\n",
    "loss_ar = []\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# num_epochs = 3\n",
    "num_epochs = 5000\n",
    "\n",
    "# loop for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # record the start time\n",
    "    start = time.time()\n",
    "    \n",
    "    # get the random training batch\n",
    "    input_batch, output_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    # to Tensor for embedding purpose\n",
    "    input_tensor = torch.LongTensor(input_batch)   #[batch_size, 1]\n",
    "    output_tensor = torch.LongTensor(output_batch) #[batch_size, 1]\n",
    "    \n",
    "    # train\n",
    "    skipgram_optimizer.zero_grad()\n",
    "    loss = skipgram_model(input_tensor, output_tensor, all_vocabs)\n",
    "    \n",
    "    # update loss\n",
    "    loss.backward()\n",
    "    skipgram_optimizer.step()\n",
    "    \n",
    "    # record the end time\n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    # record loss\n",
    "    loss_ar.append(loss)\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f\"Epoch: {epoch+1} | loss: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "        \n",
    "endtime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_training_time: 3m : 8s\n"
     ]
    }
   ],
   "source": [
    "total_training_time = epoch_time(starttime,endtime)\n",
    "print(f\"total_training_time: {total_training_time[0]}m : {total_training_time[1]}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.9912, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Plotting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the embedding given a word\n",
    "def get_embed(word):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    c_embed = skipgram_model.embedding_center(id_tensor)\n",
    "    o_embed = skipgram_model.embedding_outside(id_tensor)\n",
    "    word_embed = (c_embed + o_embed) / 2\n",
    "    x,y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6,3))\n",
    "\n",
    "# # loop each unique vocab\n",
    "# for i, word in enumerate(vocab):\n",
    "#     x,y = get_embed(word)\n",
    "#     plt.scatter(x,y)\n",
    "#     # plt.annotate(word, xy=(x,y), xytest =(5,2), textcoords='offset points')\n",
    "#     plt.annotate(word, xy = (x,y), xytext = (5,2), textcoords='offset points')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(skipgram_model.state_dict(), './model/A1-Skipgram.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "Data = {\n",
    "    'corpus': corpus,\n",
    "    'vocab': vocab,\n",
    "    'word2index': word2index,\n",
    "    'voc_size': voc_size,\n",
    "    'embedding_size': embedding_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Data,open('./model/Data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
