{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1 : Search Engine (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__, torch.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the nltk dataset = rural\n",
    "txt_file = './abc/rural.txt'\n",
    "\n",
    "with open(txt_file, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "# Split the dataset into paragraphs based on double line breaks : to get one paragraph in a list item ['paragraph1', 'paragraph']\n",
    "paragraphs = [paragraph.strip() for paragraph in text.split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(paragraphs)):\n",
    "    # Replace newline characters with spaces\n",
    "    paragraphs[i] = paragraphs[i].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. tokenization\n",
    "corpus = [sent.split(\" \") for sent in paragraphs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the origin token size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = 0\n",
    "for i in range(len(corpus)):\n",
    "    wc += len(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the machine resources limitations, take only 100 documents as my corpus\n",
    "corpus = corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the total word count in my corpus\n",
    "wc = 0\n",
    "for i in range(len(corpus)):\n",
    "    wc += len(corpus[i])\n",
    "    \n",
    "wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 numerization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique word\n",
    "\n",
    "# list comprehension for getting words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# getting unique word and store as a list\n",
    "vocab = list(set(flatten(corpus)))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <UNK> to a dictionary vocab\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization: assign index to each word\n",
    "word2index = {w:idx for idx, w in enumerate(vocab)}\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2word\n",
    "index2word = {k:v for v,k in word2index.items()}\n",
    "index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Co-occurence Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the co-occurence of two words given some window size. here, I will use ws = 2.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus)) # find the occurences of each word\n",
    "X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find co-occurence\n",
    "\n",
    "skip_grams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for i in range (2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-2],doc[i-1], doc[i+1],doc[i+2]]\n",
    "        \n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out)) # tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter (skip_grams)\n",
    "X_ik_skipgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting function\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    # check whether the co-occurences between two words: w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        \n",
    "    # if not exist, then set to 1; \"laplace smoothing\"\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    # set xmax (xmax = the maximum count of co words we will accept)\n",
    "    x_max = 100\n",
    "    \n",
    "    # set alpha (f(x))\n",
    "    alpha = 0.75 \n",
    "    \n",
    "            # note all xmax and alpha number are using according to the paper glove\n",
    "    \n",
    "    # if co-occurences does not exceed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max) ** alpha\n",
    "    \n",
    "    # otherwise set it to 1 (if xmax, set to 1)\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result # return the number between 0 and 1 which is the weighting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-occurences matrix\n",
    "\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "# a dict for keeping the co-occurences\n",
    "X_ik = {}\n",
    "\n",
    "# already scale the co-occurences using the weighting function\n",
    "weighting_dic = {}\n",
    "\n",
    "for bigram in combinations_with_replacement(vocab, 2):\n",
    "    \n",
    "    # give all the possible combination of the words\n",
    "    print(bigram) \n",
    "    \n",
    "    # if the pair exists in out corpus\n",
    "    if X_ik_skipgrams.get(bigram):\n",
    "        co                           = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram]                 = co + 1 # for stability, (if no occurence we set 1, so if occurences we need to plus 1)\n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 # basically (apple, banana = banana, apple)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram]                 = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coos, random_weightings = [], [], [], []\n",
    "    \n",
    "    # convert our skipgrams to id using word2index\n",
    "    skip_grams_id = [(word2index[skipgram[0]], word2index[skipgram[1]]) for skipgram in skip_grams]\n",
    "    # print(skip_grams_id)\n",
    "    \n",
    "    # randomly choose indexes based on batch_size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace = False)\n",
    "    \n",
    "    # get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]]) # center word, x\n",
    "        random_labels.append([skip_grams_id[index][1]]) # outside word, y\n",
    "    \n",
    "        # co occerences\n",
    "        pair = skip_grams[index] #('banana','fruit')\n",
    "        \n",
    "        try:\n",
    "            cooc = X_ik[pair]   \n",
    "        except:\n",
    "            cooc = 1\n",
    "            \n",
    "        random_coos.append([math.log(cooc)])\n",
    "    \n",
    "        # weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coos), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1)\n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1) \n",
    "        \n",
    "    def forward(self, center, outside, cooc, weighting):\n",
    "        center_embeds  = self.center_embedding(center)   # (batch_size, 1, embed_size)\n",
    "        outside_embeds = self.outside_embedding(outside) # (batch_size, 1, embed_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1) # (batch_size, voc_size)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product = outside_embeds.bmm(center_embeds.transpose(1,2)).squeeze(2)\n",
    "        # (batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - cooc, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 # so we can later plot\n",
    "voc_size       = len(vocab)\n",
    "\n",
    "model          = Glove(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time estimation\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "loss_ar = []\n",
    "starttime = time.time()\n",
    "\n",
    "# Training\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # record the starting time for each epoch\n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch     = torch.LongTensor(input_batch) #[batch_size, 1]\n",
    "    target_batch    = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "    cooc_batch      = torch.FloatTensor(cooc_batch) #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # record the ending time for each epoch\n",
    "    end = time.time()\n",
    "    \n",
    "    # cal training time for each epoch\n",
    "    epoch_mins, epoch_secs = epoch_time(start,end)\n",
    "    \n",
    "    # record loss\n",
    "    loss_ar.append(loss)\n",
    "    \n",
    "    # print loss\n",
    "    if(epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch+1:6.0f} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "        \n",
    "        \n",
    "endtime = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_training_time = epoch_time(starttime,endtime)\n",
    "print(f\"total_training_time: {total_training_time[0]}m : {total_training_time[1]}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to get embedding given a word\n",
    "def get_embed(word):\n",
    "    id_tensor  = torch.LongTensor([word2index[word]])\n",
    "    v_embed    = model.center_embedding(id_tensor)\n",
    "    u_embed    = model.outside_embedding(id_tensor)\n",
    "    word_embed = (v_embed + u_embed) / 2\n",
    "    x, y       = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), './model/A1-Glove.pt')\n",
    "\n",
    "# save the data\n",
    "Data = {\n",
    "    'corpus': corpus,\n",
    "    'vocab': vocab,\n",
    "    'word2index': word2index,\n",
    "    'voc_size': voc_size,\n",
    "    'embedding_size': embedding_size\n",
    "}\n",
    "pickle.dump(Data,open('./model/Data.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
