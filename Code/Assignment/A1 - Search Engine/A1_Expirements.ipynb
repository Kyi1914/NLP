{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirements for Task 2\n",
    "\n",
    "According to Task 2, I will conduct the follwings expirements:\n",
    "- (1) Compare Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time. \n",
    "- (2) Use Word analogies dataset to calucalte between syntactic and semantic accuracy, similar to the\n",
    "methods in the Word2Vec and GloVe paper. \n",
    "- (3) Use the similarity dataset to find the correlation between your modelsâ€™ dot product and the provided similarity metrics. (from scipy.stats import spearmanr) Assess if your embeddings correlate with human judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import 3 models from main.py\n",
    "from helper import Skipgram, SkipgramNeg, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "Data = pickle.load(open('./model/Data.pkl', 'rb'))\n",
    "corpus = Data['corpus']\n",
    "vocab = Data['vocab']\n",
    "word2index = Data['word2index']\n",
    "voc_size = Data['voc_size']\n",
    "embed_size = Data['embedding_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(4167, 2)\n",
       "  (embedding_outside): Embedding(4167, 2)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create object of the Skipgram model and load parameters\n",
    "Skipgram = Skipgram(voc_size, embed_size)\n",
    "Skipgram.load_state_dict(torch.load('model/A1-Skipgram.pt'))\n",
    "Skipgram.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object of the SkipgramNeg model and load parameters\n",
    "SkipgramNeg = SkipgramNeg(voc_size, embed_size)\n",
    "SkipgramNeg.load_state_dict(torch.load('model/A1-NegSampling.pt'))\n",
    "SkipgramNeg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and load saved parameters\n",
    "glove = Glove(voc_size, embed_size)\n",
    "glove.load_state_dict(torch.load('model/A1-Glove.pt'))\n",
    "glove.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to put this file in some python/gensim directory; just run it and it will inform where to put\n",
    "glove_file = ('./glove.6B/glove.6B.100d.txt')\n",
    "gensim = KeyedVectors.load_word2vec_format(glove_file, binary = False, no_header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function for semantic and syntactic analysis \n",
    "that calculates similarities using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_similarity(word_vectors, result_vector):\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = F.cosine_similarity(result_vector, word_vectors)\n",
    "\n",
    "    # Find the index of the word with the highest similarity\n",
    "    closest_word_index = torch.argmax(similarities).item()\n",
    "\n",
    "    return closest_word_index\n",
    "\n",
    "def similarities(lines, model, vocab):\n",
    "    \n",
    "    # Get word vectors for all words in the vocabulary\n",
    "    all_word_vectors = torch.stack([model.get_embed(word) for word in vocab])\n",
    "    # [voc_size, 1, emb_size]\n",
    "\n",
    "    correct_count = 0\n",
    "    # Perform vector manipulation for each set of four words\n",
    "    for line in lines: # ['dancing', 'danced', 'falling', 'fell']\n",
    "        words = line\n",
    "        \n",
    "        # Assuming there are four words in each line\n",
    "        vectors = [model.get_embed(word) if word in vocab else model.get_embed('<UNK>') for word in words]\n",
    "        \n",
    "        # Perform vector manipulation (e.g., subtraction, addition)\n",
    "        result_vector = vectors[1][0] - vectors[0][0] + vectors[2][0] # [, emb_size]\n",
    "\n",
    "        # Add a batch dimension to result_vector\n",
    "        result_vector = result_vector.unsqueeze(0) # [1, emb_size]\n",
    "\n",
    "        # Find the closest word index using cosine similarity\n",
    "        closest_word_index = calculate_similarity(all_word_vectors, result_vector)\n",
    "\n",
    "        # Get the closest word from your vocabulary\n",
    "        closest_word = vocab[closest_word_index]\n",
    "\n",
    "        if closest_word == words[3]:\n",
    "            correct_count += 1\n",
    "            \n",
    "    return (correct_count / len(lines)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing data, I will use Word analogies dataset to calucalte between syntactic and semantic accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to file\n",
    "txt_file = '../A1 - Search Engine/word-test.v1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': 'capital-common-countries', 'pairs': [['Athens', 'Greece', 'Baghdad', 'Iraq'], ['Athens', 'Greece', 'Bangkok', 'Thailand'], ['Athens', 'Greece', 'Beijing', 'China'], ['Athens', 'Greece', 'Berlin', 'Germany'], ['Athens', 'Greece', 'Bern', 'Switzerland'], ['Athens', 'Greece', 'Cairo', 'Egypt'], ['Athens', 'Greece', 'Canberra', 'Australia'], ['Athens', 'Greece', 'Hanoi', 'Vietnam'], ['Athens', 'Greece', 'Havana', 'Cuba'], ['Athens', 'Greece', 'Helsinki', 'Finland'], ['Athens', 'Greece', 'Islamabad', 'Pakistan'], ['Athens', 'Greece', 'Kabul', 'Afghanistan'], ['Athens', 'Greece', 'London', 'England'], ['Athens', 'Greece', 'Madrid', 'Spain'], ['Athens', 'Greece', 'Moscow', 'Russia'], ['Athens', 'Greece', 'Oslo', 'Norway'], ['Athens', 'Greece', 'Ottawa', 'Canada'], ['Athens', 'Greece', 'Paris', 'France'], ['Athens', 'Greece', 'Rome', 'Italy'], ['Athens', 'Greece', 'Stockholm', 'Sweden'], ['Athens', 'Greece', 'Tehran', 'Iran'], ['Athens', 'Greece', 'Tokyo', 'Japan'], ['Baghdad', 'Iraq', 'Bangkok', 'Thailand'], ['Baghdad', 'Iraq', 'Beijing', 'China'], ['Baghdad', 'Iraq', 'Berlin', 'Germany'], ['Baghdad', 'Iraq', 'Bern', 'Switzerland'], ['Baghdad', 'Iraq', 'Cairo', 'Egypt'], ['Baghdad', 'Iraq', 'Canberra', 'Australia'], ['Baghdad', 'Iraq', 'Hanoi', 'Vietnam'], ['Baghdad', 'Iraq', 'Havana', 'Cuba'], ['Baghdad', 'Iraq', 'Helsinki', 'Finland'], ['Baghdad', 'Iraq', 'Islamabad', 'Pakistan'], ['Baghdad', 'Iraq', 'Kabul', 'Afghanistan'], ['Baghdad', 'Iraq', 'London', 'England'], ['Baghdad', 'Iraq', 'Madrid', 'Spain'], ['Baghdad', 'Iraq', 'Moscow', 'Russia'], ['Baghdad', 'Iraq', 'Oslo', 'Norway'], ['Baghdad', 'Iraq', 'Ottawa', 'Canada'], ['Baghdad', 'Iraq', 'Paris', 'France'], ['Baghdad', 'Iraq', 'Rome', 'Italy'], ['Baghdad', 'Iraq', 'Stockholm', 'Sweden'], ['Baghdad', 'Iraq', 'Tehran', 'Iran'], ['Baghdad', 'Iraq', 'Tokyo', 'Japan'], ['Baghdad', 'Iraq', 'Athens', 'Greece'], ['Bangkok', 'Thailand', 'Beijing', 'China'], ['Bangkok', 'Thailand', 'Berlin', 'Germany'], ['Bangkok', 'Thailand', 'Bern', 'Switzerland'], ['Bangkok', 'Thailand', 'Cairo', 'Egypt'], ['Bangkok', 'Thailand', 'Canberra', 'Australia'], ['Bangkok', 'Thailand', 'Hanoi', 'Vietnam'], ['Bangkok', 'Thailand', 'Havana', 'Cuba'], ['Bangkok', 'Thailand', 'Helsinki', 'Finland'], ['Bangkok', 'Thailand', 'Islamabad', 'Pakistan'], ['Bangkok', 'Thailand', 'Kabul', 'Afghanistan'], ['Bangkok', 'Thailand', 'London', 'England'], ['Bangkok', 'Thailand', 'Madrid', 'Spain'], ['Bangkok', 'Thailand', 'Moscow', 'Russia'], ['Bangkok', 'Thailand', 'Oslo', 'Norway'], ['Bangkok', 'Thailand', 'Ottawa', 'Canada'], ['Bangkok', 'Thailand', 'Paris', 'France'], ['Bangkok', 'Thailand', 'Rome', 'Italy'], ['Bangkok', 'Thailand', 'Stockholm', 'Sweden'], ['Bangkok', 'Thailand', 'Tehran', 'Iran'], ['Bangkok', 'Thailand', 'Tokyo', 'Japan'], ['Bangkok', 'Thailand', 'Athens', 'Greece'], ['Bangkok', 'Thailand', 'Baghdad', 'Iraq'], ['Beijing', 'China', 'Berlin', 'Germany'], ['Beijing', 'China', 'Bern', 'Switzerland'], ['Beijing', 'China', 'Cairo', 'Egypt'], ['Beijing', 'China', 'Canberra', 'Australia'], ['Beijing', 'China', 'Hanoi', 'Vietnam'], ['Beijing', 'China', 'Havana', 'Cuba'], ['Beijing', 'China', 'Helsinki', 'Finland'], ['Beijing', 'China', 'Islamabad', 'Pakistan'], ['Beijing', 'China', 'Kabul', 'Afghanistan'], ['Beijing', 'China', 'London', 'England'], ['Beijing', 'China', 'Madrid', 'Spain'], ['Beijing', 'China', 'Moscow', 'Russia'], ['Beijing', 'China', 'Oslo', 'Norway'], ['Beijing', 'China', 'Ottawa', 'Canada'], ['Beijing', 'China', 'Paris', 'France'], ['Beijing', 'China', 'Rome', 'Italy'], ['Beijing', 'China', 'Stockholm', 'Sweden'], ['Beijing', 'China', 'Tehran', 'Iran'], ['Beijing', 'China', 'Tokyo', 'Japan'], ['Beijing', 'China', 'Athens', 'Greece'], ['Beijing', 'China', 'Baghdad', 'Iraq'], ['Beijing', 'China', 'Bangkok', 'Thailand'], ['Berlin', 'Germany', 'Bern', 'Switzerland'], ['Berlin', 'Germany', 'Cairo', 'Egypt'], ['Berlin', 'Germany', 'Canberra', 'Australia'], ['Berlin', 'Germany', 'Hanoi', 'Vietnam'], ['Berlin', 'Germany', 'Havana', 'Cuba'], ['Berlin', 'Germany', 'Helsinki', 'Finland'], ['Berlin', 'Germany', 'Islamabad', 'Pakistan'], ['Berlin', 'Germany', 'Kabul', 'Afghanistan'], ['Berlin', 'Germany', 'London', 'England'], ['Berlin', 'Germany', 'Madrid', 'Spain'], ['Berlin', 'Germany', 'Moscow', 'Russia'], ['Berlin', 'Germany', 'Oslo', 'Norway'], ['Berlin', 'Germany', 'Ottawa', 'Canada'], ['Berlin', 'Germany', 'Paris', 'France'], ['Berlin', 'Germany', 'Rome', 'Italy'], ['Berlin', 'Germany', 'Stockholm', 'Sweden'], ['Berlin', 'Germany', 'Tehran', 'Iran'], ['Berlin', 'Germany', 'Tokyo', 'Japan'], ['Berlin', 'Germany', 'Athens', 'Greece'], ['Berlin', 'Germany', 'Baghdad', 'Iraq'], ['Berlin', 'Germany', 'Bangkok', 'Thailand'], ['Berlin', 'Germany', 'Beijing', 'China'], ['Bern', 'Switzerland', 'Cairo', 'Egypt'], ['Bern', 'Switzerland', 'Canberra', 'Australia'], ['Bern', 'Switzerland', 'Hanoi', 'Vietnam'], ['Bern', 'Switzerland', 'Havana', 'Cuba'], ['Bern', 'Switzerland', 'Helsinki', 'Finland'], ['Bern', 'Switzerland', 'Islamabad', 'Pakistan'], ['Bern', 'Switzerland', 'Kabul', 'Afghanistan'], ['Bern', 'Switzerland', 'London', 'England'], ['Bern', 'Switzerland', 'Madrid', 'Spain'], ['Bern', 'Switzerland', 'Moscow', 'Russia'], ['Bern', 'Switzerland', 'Oslo', 'Norway'], ['Bern', 'Switzerland', 'Ottawa', 'Canada'], ['Bern', 'Switzerland', 'Paris', 'France'], ['Bern', 'Switzerland', 'Rome', 'Italy'], ['Bern', 'Switzerland', 'Stockholm', 'Sweden'], ['Bern', 'Switzerland', 'Tehran', 'Iran'], ['Bern', 'Switzerland', 'Tokyo', 'Japan'], ['Bern', 'Switzerland', 'Athens', 'Greece'], ['Bern', 'Switzerland', 'Baghdad', 'Iraq'], ['Bern', 'Switzerland', 'Bangkok', 'Thailand'], ['Bern', 'Switzerland', 'Beijing', 'China'], ['Bern', 'Switzerland', 'Berlin', 'Germany'], ['Cairo', 'Egypt', 'Canberra', 'Australia'], ['Cairo', 'Egypt', 'Hanoi', 'Vietnam'], ['Cairo', 'Egypt', 'Havana', 'Cuba'], ['Cairo', 'Egypt', 'Helsinki', 'Finland'], ['Cairo', 'Egypt', 'Islamabad', 'Pakistan'], ['Cairo', 'Egypt', 'Kabul', 'Afghanistan'], ['Cairo', 'Egypt', 'London', 'England'], ['Cairo', 'Egypt', 'Madrid', 'Spain'], ['Cairo', 'Egypt', 'Moscow', 'Russia'], ['Cairo', 'Egypt', 'Oslo', 'Norway'], ['Cairo', 'Egypt', 'Ottawa', 'Canada'], ['Cairo', 'Egypt', 'Paris', 'France'], ['Cairo', 'Egypt', 'Rome', 'Italy'], ['Cairo', 'Egypt', 'Stockholm', 'Sweden'], ['Cairo', 'Egypt', 'Tehran', 'Iran'], ['Cairo', 'Egypt', 'Tokyo', 'Japan'], ['Cairo', 'Egypt', 'Athens', 'Greece'], ['Cairo', 'Egypt', 'Baghdad', 'Iraq'], ['Cairo', 'Egypt', 'Bangkok', 'Thailand'], ['Cairo', 'Egypt', 'Beijing', 'China'], ['Cairo', 'Egypt', 'Berlin', 'Germany'], ['Cairo', 'Egypt', 'Bern', 'Switzerland'], ['Canberra', 'Australia', 'Hanoi', 'Vietnam'], ['Canberra', 'Australia', 'Havana', 'Cuba'], ['Canberra', 'Australia', 'Helsinki', 'Finland'], ['Canberra', 'Australia', 'Islamabad', 'Pakistan'], ['Canberra', 'Australia', 'Kabul', 'Afghanistan'], ['Canberra', 'Australia', 'London', 'England'], ['Canberra', 'Australia', 'Madrid', 'Spain'], ['Canberra', 'Australia', 'Moscow', 'Russia'], ['Canberra', 'Australia', 'Oslo', 'Norway'], ['Canberra', 'Australia', 'Ottawa', 'Canada'], ['Canberra', 'Australia', 'Paris', 'France'], ['Canberra', 'Australia', 'Rome', 'Italy'], ['Canberra', 'Australia', 'Stockholm', 'Sweden'], ['Canberra', 'Australia', 'Tehran', 'Iran'], ['Canberra', 'Australia', 'Tokyo', 'Japan'], ['Canberra', 'Australia', 'Athens', 'Greece'], ['Canberra', 'Australia', 'Baghdad', 'Iraq'], ['Canberra', 'Australia', 'Bangkok', 'Thailand'], ['Canberra', 'Australia', 'Beijing', 'China'], ['Canberra', 'Australia', 'Berlin', 'Germany'], ['Canberra', 'Australia', 'Bern', 'Switzerland'], ['Canberra', 'Australia', 'Cairo', 'Egypt'], ['Hanoi', 'Vietnam', 'Havana', 'Cuba'], ['Hanoi', 'Vietnam', 'Helsinki', 'Finland'], ['Hanoi', 'Vietnam', 'Islamabad', 'Pakistan'], ['Hanoi', 'Vietnam', 'Kabul', 'Afghanistan'], ['Hanoi', 'Vietnam', 'London', 'England'], ['Hanoi', 'Vietnam', 'Madrid', 'Spain'], ['Hanoi', 'Vietnam', 'Moscow', 'Russia'], ['Hanoi', 'Vietnam', 'Oslo', 'Norway'], ['Hanoi', 'Vietnam', 'Ottawa', 'Canada'], ['Hanoi', 'Vietnam', 'Paris', 'France'], ['Hanoi', 'Vietnam', 'Rome', 'Italy'], ['Hanoi', 'Vietnam', 'Stockholm', 'Sweden'], ['Hanoi', 'Vietnam', 'Tehran', 'Iran'], ['Hanoi', 'Vietnam', 'Tokyo', 'Japan'], ['Hanoi', 'Vietnam', 'Athens', 'Greece'], ['Hanoi', 'Vietnam', 'Baghdad', 'Iraq'], ['Hanoi', 'Vietnam', 'Bangkok', 'Thailand'], ['Hanoi', 'Vietnam', 'Beijing', 'China'], ['Hanoi', 'Vietnam', 'Berlin', 'Germany'], ['Hanoi', 'Vietnam', 'Bern', 'Switzerland'], ['Hanoi', 'Vietnam', 'Cairo', 'Egypt'], ['Hanoi', 'Vietnam', 'Canberra', 'Australia'], ['Havana', 'Cuba', 'Helsinki', 'Finland'], ['Havana', 'Cuba', 'Islamabad', 'Pakistan'], ['Havana', 'Cuba', 'Kabul', 'Afghanistan'], ['Havana', 'Cuba', 'London', 'England'], ['Havana', 'Cuba', 'Madrid', 'Spain'], ['Havana', 'Cuba', 'Moscow', 'Russia'], ['Havana', 'Cuba', 'Oslo', 'Norway'], ['Havana', 'Cuba', 'Ottawa', 'Canada'], ['Havana', 'Cuba', 'Paris', 'France'], ['Havana', 'Cuba', 'Rome', 'Italy'], ['Havana', 'Cuba', 'Stockholm', 'Sweden'], ['Havana', 'Cuba', 'Tehran', 'Iran'], ['Havana', 'Cuba', 'Tokyo', 'Japan'], ['Havana', 'Cuba', 'Athens', 'Greece'], ['Havana', 'Cuba', 'Baghdad', 'Iraq'], ['Havana', 'Cuba', 'Bangkok', 'Thailand'], ['Havana', 'Cuba', 'Beijing', 'China'], ['Havana', 'Cuba', 'Berlin', 'Germany'], ['Havana', 'Cuba', 'Bern', 'Switzerland'], ['Havana', 'Cuba', 'Cairo', 'Egypt'], ['Havana', 'Cuba', 'Canberra', 'Australia'], ['Havana', 'Cuba', 'Hanoi', 'Vietnam'], ['Helsinki', 'Finland', 'Islamabad', 'Pakistan'], ['Helsinki', 'Finland', 'Kabul', 'Afghanistan'], ['Helsinki', 'Finland', 'London', 'England'], ['Helsinki', 'Finland', 'Madrid', 'Spain'], ['Helsinki', 'Finland', 'Moscow', 'Russia'], ['Helsinki', 'Finland', 'Oslo', 'Norway'], ['Helsinki', 'Finland', 'Ottawa', 'Canada'], ['Helsinki', 'Finland', 'Paris', 'France'], ['Helsinki', 'Finland', 'Rome', 'Italy'], ['Helsinki', 'Finland', 'Stockholm', 'Sweden'], ['Helsinki', 'Finland', 'Tehran', 'Iran'], ['Helsinki', 'Finland', 'Tokyo', 'Japan'], ['Helsinki', 'Finland', 'Athens', 'Greece'], ['Helsinki', 'Finland', 'Baghdad', 'Iraq'], ['Helsinki', 'Finland', 'Bangkok', 'Thailand'], ['Helsinki', 'Finland', 'Beijing', 'China'], ['Helsinki', 'Finland', 'Berlin', 'Germany'], ['Helsinki', 'Finland', 'Bern', 'Switzerland'], ['Helsinki', 'Finland', 'Cairo', 'Egypt'], ['Helsinki', 'Finland', 'Canberra', 'Australia'], ['Helsinki', 'Finland', 'Hanoi', 'Vietnam'], ['Helsinki', 'Finland', 'Havana', 'Cuba'], ['Islamabad', 'Pakistan', 'Kabul', 'Afghanistan'], ['Islamabad', 'Pakistan', 'London', 'England'], ['Islamabad', 'Pakistan', 'Madrid', 'Spain'], ['Islamabad', 'Pakistan', 'Moscow', 'Russia'], ['Islamabad', 'Pakistan', 'Oslo', 'Norway'], ['Islamabad', 'Pakistan', 'Ottawa', 'Canada'], ['Islamabad', 'Pakistan', 'Paris', 'France'], ['Islamabad', 'Pakistan', 'Rome', 'Italy'], ['Islamabad', 'Pakistan', 'Stockholm', 'Sweden'], ['Islamabad', 'Pakistan', 'Tehran', 'Iran'], ['Islamabad', 'Pakistan', 'Tokyo', 'Japan'], ['Islamabad', 'Pakistan', 'Athens', 'Greece'], ['Islamabad', 'Pakistan', 'Baghdad', 'Iraq'], ['Islamabad', 'Pakistan', 'Bangkok', 'Thailand'], ['Islamabad', 'Pakistan', 'Beijing', 'China'], ['Islamabad', 'Pakistan', 'Berlin', 'Germany'], ['Islamabad', 'Pakistan', 'Bern', 'Switzerland'], ['Islamabad', 'Pakistan', 'Cairo', 'Egypt'], ['Islamabad', 'Pakistan', 'Canberra', 'Australia'], ['Islamabad', 'Pakistan', 'Hanoi', 'Vietnam'], ['Islamabad', 'Pakistan', 'Havana', 'Cuba'], ['Islamabad', 'Pakistan', 'Helsinki', 'Finland'], ['Kabul', 'Afghanistan', 'London', 'England'], ['Kabul', 'Afghanistan', 'Madrid', 'Spain'], ['Kabul', 'Afghanistan', 'Moscow', 'Russia'], ['Kabul', 'Afghanistan', 'Oslo', 'Norway'], ['Kabul', 'Afghanistan', 'Ottawa', 'Canada'], ['Kabul', 'Afghanistan', 'Paris', 'France'], ['Kabul', 'Afghanistan', 'Rome', 'Italy'], ['Kabul', 'Afghanistan', 'Stockholm', 'Sweden'], ['Kabul', 'Afghanistan', 'Tehran', 'Iran'], ['Kabul', 'Afghanistan', 'Tokyo', 'Japan'], ['Kabul', 'Afghanistan', 'Athens', 'Greece'], ['Kabul', 'Afghanistan', 'Baghdad', 'Iraq'], ['Kabul', 'Afghanistan', 'Bangkok', 'Thailand'], ['Kabul', 'Afghanistan', 'Beijing', 'China'], ['Kabul', 'Afghanistan', 'Berlin', 'Germany'], ['Kabul', 'Afghanistan', 'Bern', 'Switzerland'], ['Kabul', 'Afghanistan', 'Cairo', 'Egypt'], ['Kabul', 'Afghanistan', 'Canberra', 'Australia'], ['Kabul', 'Afghanistan', 'Hanoi', 'Vietnam'], ['Kabul', 'Afghanistan', 'Havana', 'Cuba'], ['Kabul', 'Afghanistan', 'Helsinki', 'Finland'], ['Kabul', 'Afghanistan', 'Islamabad', 'Pakistan'], ['London', 'England', 'Madrid', 'Spain'], ['London', 'England', 'Moscow', 'Russia'], ['London', 'England', 'Oslo', 'Norway'], ['London', 'England', 'Ottawa', 'Canada'], ['London', 'England', 'Paris', 'France'], ['London', 'England', 'Rome', 'Italy'], ['London', 'England', 'Stockholm', 'Sweden'], ['London', 'England', 'Tehran', 'Iran'], ['London', 'England', 'Tokyo', 'Japan'], ['London', 'England', 'Athens', 'Greece'], ['London', 'England', 'Baghdad', 'Iraq'], ['London', 'England', 'Bangkok', 'Thailand'], ['London', 'England', 'Beijing', 'China'], ['London', 'England', 'Berlin', 'Germany'], ['London', 'England', 'Bern', 'Switzerland'], ['London', 'England', 'Cairo', 'Egypt'], ['London', 'England', 'Canberra', 'Australia'], ['London', 'England', 'Hanoi', 'Vietnam'], ['London', 'England', 'Havana', 'Cuba'], ['London', 'England', 'Helsinki', 'Finland'], ['London', 'England', 'Islamabad', 'Pakistan'], ['London', 'England', 'Kabul', 'Afghanistan'], ['Madrid', 'Spain', 'Moscow', 'Russia'], ['Madrid', 'Spain', 'Oslo', 'Norway'], ['Madrid', 'Spain', 'Ottawa', 'Canada'], ['Madrid', 'Spain', 'Paris', 'France'], ['Madrid', 'Spain', 'Rome', 'Italy'], ['Madrid', 'Spain', 'Stockholm', 'Sweden'], ['Madrid', 'Spain', 'Tehran', 'Iran'], ['Madrid', 'Spain', 'Tokyo', 'Japan'], ['Madrid', 'Spain', 'Athens', 'Greece'], ['Madrid', 'Spain', 'Baghdad', 'Iraq'], ['Madrid', 'Spain', 'Bangkok', 'Thailand'], ['Madrid', 'Spain', 'Beijing', 'China'], ['Madrid', 'Spain', 'Berlin', 'Germany'], ['Madrid', 'Spain', 'Bern', 'Switzerland'], ['Madrid', 'Spain', 'Cairo', 'Egypt'], ['Madrid', 'Spain', 'Canberra', 'Australia'], ['Madrid', 'Spain', 'Hanoi', 'Vietnam'], ['Madrid', 'Spain', 'Havana', 'Cuba'], ['Madrid', 'Spain', 'Helsinki', 'Finland'], ['Madrid', 'Spain', 'Islamabad', 'Pakistan'], ['Madrid', 'Spain', 'Kabul', 'Afghanistan'], ['Madrid', 'Spain', 'London', 'England'], ['Moscow', 'Russia', 'Oslo', 'Norway'], ['Moscow', 'Russia', 'Ottawa', 'Canada'], ['Moscow', 'Russia', 'Paris', 'France'], ['Moscow', 'Russia', 'Rome', 'Italy'], ['Moscow', 'Russia', 'Stockholm', 'Sweden'], ['Moscow', 'Russia', 'Tehran', 'Iran'], ['Moscow', 'Russia', 'Tokyo', 'Japan'], ['Moscow', 'Russia', 'Athens', 'Greece'], ['Moscow', 'Russia', 'Baghdad', 'Iraq'], ['Moscow', 'Russia', 'Bangkok', 'Thailand'], ['Moscow', 'Russia', 'Beijing', 'China'], ['Moscow', 'Russia', 'Berlin', 'Germany'], ['Moscow', 'Russia', 'Bern', 'Switzerland'], ['Moscow', 'Russia', 'Cairo', 'Egypt'], ['Moscow', 'Russia', 'Canberra', 'Australia'], ['Moscow', 'Russia', 'Hanoi', 'Vietnam'], ['Moscow', 'Russia', 'Havana', 'Cuba'], ['Moscow', 'Russia', 'Helsinki', 'Finland'], ['Moscow', 'Russia', 'Islamabad', 'Pakistan'], ['Moscow', 'Russia', 'Kabul', 'Afghanistan'], ['Moscow', 'Russia', 'London', 'England'], ['Moscow', 'Russia', 'Madrid', 'Spain'], ['Oslo', 'Norway', 'Ottawa', 'Canada'], ['Oslo', 'Norway', 'Paris', 'France'], ['Oslo', 'Norway', 'Rome', 'Italy'], ['Oslo', 'Norway', 'Stockholm', 'Sweden'], ['Oslo', 'Norway', 'Tehran', 'Iran'], ['Oslo', 'Norway', 'Tokyo', 'Japan'], ['Oslo', 'Norway', 'Athens', 'Greece'], ['Oslo', 'Norway', 'Baghdad', 'Iraq'], ['Oslo', 'Norway', 'Bangkok', 'Thailand'], ['Oslo', 'Norway', 'Beijing', 'China'], ['Oslo', 'Norway', 'Berlin', 'Germany'], ['Oslo', 'Norway', 'Bern', 'Switzerland'], ['Oslo', 'Norway', 'Cairo', 'Egypt'], ['Oslo', 'Norway', 'Canberra', 'Australia'], ['Oslo', 'Norway', 'Hanoi', 'Vietnam'], ['Oslo', 'Norway', 'Havana', 'Cuba'], ['Oslo', 'Norway', 'Helsinki', 'Finland'], ['Oslo', 'Norway', 'Islamabad', 'Pakistan'], ['Oslo', 'Norway', 'Kabul', 'Afghanistan'], ['Oslo', 'Norway', 'London', 'England'], ['Oslo', 'Norway', 'Madrid', 'Spain'], ['Oslo', 'Norway', 'Moscow', 'Russia'], ['Ottawa', 'Canada', 'Paris', 'France'], ['Ottawa', 'Canada', 'Rome', 'Italy'], ['Ottawa', 'Canada', 'Stockholm', 'Sweden'], ['Ottawa', 'Canada', 'Tehran', 'Iran'], ['Ottawa', 'Canada', 'Tokyo', 'Japan'], ['Ottawa', 'Canada', 'Athens', 'Greece'], ['Ottawa', 'Canada', 'Baghdad', 'Iraq'], ['Ottawa', 'Canada', 'Bangkok', 'Thailand'], ['Ottawa', 'Canada', 'Beijing', 'China'], ['Ottawa', 'Canada', 'Berlin', 'Germany'], ['Ottawa', 'Canada', 'Bern', 'Switzerland'], ['Ottawa', 'Canada', 'Cairo', 'Egypt'], ['Ottawa', 'Canada', 'Canberra', 'Australia'], ['Ottawa', 'Canada', 'Hanoi', 'Vietnam'], ['Ottawa', 'Canada', 'Havana', 'Cuba'], ['Ottawa', 'Canada', 'Helsinki', 'Finland'], ['Ottawa', 'Canada', 'Islamabad', 'Pakistan'], ['Ottawa', 'Canada', 'Kabul', 'Afghanistan'], ['Ottawa', 'Canada', 'London', 'England'], ['Ottawa', 'Canada', 'Madrid', 'Spain'], ['Ottawa', 'Canada', 'Moscow', 'Russia'], ['Ottawa', 'Canada', 'Oslo', 'Norway'], ['Paris', 'France', 'Rome', 'Italy'], ['Paris', 'France', 'Stockholm', 'Sweden'], ['Paris', 'France', 'Tehran', 'Iran'], ['Paris', 'France', 'Tokyo', 'Japan'], ['Paris', 'France', 'Athens', 'Greece'], ['Paris', 'France', 'Baghdad', 'Iraq'], ['Paris', 'France', 'Bangkok', 'Thailand'], ['Paris', 'France', 'Beijing', 'China'], ['Paris', 'France', 'Berlin', 'Germany'], ['Paris', 'France', 'Bern', 'Switzerland'], ['Paris', 'France', 'Cairo', 'Egypt'], ['Paris', 'France', 'Canberra', 'Australia'], ['Paris', 'France', 'Hanoi', 'Vietnam'], ['Paris', 'France', 'Havana', 'Cuba'], ['Paris', 'France', 'Helsinki', 'Finland'], ['Paris', 'France', 'Islamabad', 'Pakistan'], ['Paris', 'France', 'Kabul', 'Afghanistan'], ['Paris', 'France', 'London', 'England'], ['Paris', 'France', 'Madrid', 'Spain'], ['Paris', 'France', 'Moscow', 'Russia'], ['Paris', 'France', 'Oslo', 'Norway'], ['Paris', 'France', 'Ottawa', 'Canada'], ['Rome', 'Italy', 'Stockholm', 'Sweden'], ['Rome', 'Italy', 'Tehran', 'Iran'], ['Rome', 'Italy', 'Tokyo', 'Japan'], ['Rome', 'Italy', 'Athens', 'Greece'], ['Rome', 'Italy', 'Baghdad', 'Iraq'], ['Rome', 'Italy', 'Bangkok', 'Thailand'], ['Rome', 'Italy', 'Beijing', 'China'], ['Rome', 'Italy', 'Berlin', 'Germany'], ['Rome', 'Italy', 'Bern', 'Switzerland'], ['Rome', 'Italy', 'Cairo', 'Egypt'], ['Rome', 'Italy', 'Canberra', 'Australia'], ['Rome', 'Italy', 'Hanoi', 'Vietnam'], ['Rome', 'Italy', 'Havana', 'Cuba'], ['Rome', 'Italy', 'Helsinki', 'Finland'], ['Rome', 'Italy', 'Islamabad', 'Pakistan'], ['Rome', 'Italy', 'Kabul', 'Afghanistan'], ['Rome', 'Italy', 'London', 'England'], ['Rome', 'Italy', 'Madrid', 'Spain'], ['Rome', 'Italy', 'Moscow', 'Russia'], ['Rome', 'Italy', 'Oslo', 'Norway'], ['Rome', 'Italy', 'Ottawa', 'Canada'], ['Rome', 'Italy', 'Paris', 'France'], ['Stockholm', 'Sweden', 'Tehran', 'Iran'], ['Stockholm', 'Sweden', 'Tokyo', 'Japan'], ['Stockholm', 'Sweden', 'Athens', 'Greece'], ['Stockholm', 'Sweden', 'Baghdad', 'Iraq'], ['Stockholm', 'Sweden', 'Bangkok', 'Thailand'], ['Stockholm', 'Sweden', 'Beijing', 'China'], ['Stockholm', 'Sweden', 'Berlin', 'Germany'], ['Stockholm', 'Sweden', 'Bern', 'Switzerland'], ['Stockholm', 'Sweden', 'Cairo', 'Egypt'], ['Stockholm', 'Sweden', 'Canberra', 'Australia'], ['Stockholm', 'Sweden', 'Hanoi', 'Vietnam'], ['Stockholm', 'Sweden', 'Havana', 'Cuba'], ['Stockholm', 'Sweden', 'Helsinki', 'Finland'], ['Stockholm', 'Sweden', 'Islamabad', 'Pakistan'], ['Stockholm', 'Sweden', 'Kabul', 'Afghanistan'], ['Stockholm', 'Sweden', 'London', 'England'], ['Stockholm', 'Sweden', 'Madrid', 'Spain'], ['Stockholm', 'Sweden', 'Moscow', 'Russia'], ['Stockholm', 'Sweden', 'Oslo', 'Norway'], ['Stockholm', 'Sweden', 'Ottawa', 'Canada'], ['Stockholm', 'Sweden', 'Paris', 'France'], ['Stockholm', 'Sweden', 'Rome', 'Italy'], ['Tehran', 'Iran', 'Tokyo', 'Japan'], ['Tehran', 'Iran', 'Athens', 'Greece'], ['Tehran', 'Iran', 'Baghdad', 'Iraq'], ['Tehran', 'Iran', 'Bangkok', 'Thailand'], ['Tehran', 'Iran', 'Beijing', 'China'], ['Tehran', 'Iran', 'Berlin', 'Germany'], ['Tehran', 'Iran', 'Bern', 'Switzerland'], ['Tehran', 'Iran', 'Cairo', 'Egypt'], ['Tehran', 'Iran', 'Canberra', 'Australia'], ['Tehran', 'Iran', 'Hanoi', 'Vietnam'], ['Tehran', 'Iran', 'Havana', 'Cuba'], ['Tehran', 'Iran', 'Helsinki', 'Finland'], ['Tehran', 'Iran', 'Islamabad', 'Pakistan'], ['Tehran', 'Iran', 'Kabul', 'Afghanistan'], ['Tehran', 'Iran', 'London', 'England'], ['Tehran', 'Iran', 'Madrid', 'Spain'], ['Tehran', 'Iran', 'Moscow', 'Russia'], ['Tehran', 'Iran', 'Oslo', 'Norway'], ['Tehran', 'Iran', 'Ottawa', 'Canada'], ['Tehran', 'Iran', 'Paris', 'France'], ['Tehran', 'Iran', 'Rome', 'Italy'], ['Tehran', 'Iran', 'Stockholm', 'Sweden'], ['Tokyo', 'Japan', 'Athens', 'Greece'], ['Tokyo', 'Japan', 'Baghdad', 'Iraq'], ['Tokyo', 'Japan', 'Bangkok', 'Thailand'], ['Tokyo', 'Japan', 'Beijing', 'China'], ['Tokyo', 'Japan', 'Berlin', 'Germany'], ['Tokyo', 'Japan', 'Bern', 'Switzerland'], ['Tokyo', 'Japan', 'Cairo', 'Egypt'], ['Tokyo', 'Japan', 'Canberra', 'Australia'], ['Tokyo', 'Japan', 'Hanoi', 'Vietnam'], ['Tokyo', 'Japan', 'Havana', 'Cuba'], ['Tokyo', 'Japan', 'Helsinki', 'Finland'], ['Tokyo', 'Japan', 'Islamabad', 'Pakistan'], ['Tokyo', 'Japan', 'Kabul', 'Afghanistan'], ['Tokyo', 'Japan', 'London', 'England'], ['Tokyo', 'Japan', 'Madrid', 'Spain'], ['Tokyo', 'Japan', 'Moscow', 'Russia'], ['Tokyo', 'Japan', 'Oslo', 'Norway'], ['Tokyo', 'Japan', 'Ottawa', 'Canada'], ['Tokyo', 'Japan', 'Paris', 'France'], ['Tokyo', 'Japan', 'Rome', 'Italy'], ['Tokyo', 'Japan', 'Stockholm', 'Sweden'], ['Tokyo', 'Japan', 'Tehran', 'Iran']]}\n"
     ]
    }
   ],
   "source": [
    "analogy = []\n",
    "# current_analogy_set = {}\n",
    "\n",
    "with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "    \n",
    "    # Read all lines from the file and store them in a list\n",
    "    lines = file.readlines()\n",
    "\n",
    "i = 0\n",
    "# Iterate through lines and extract analogy sets\n",
    "for line in lines:\n",
    "    line = line.strip() # remove the space\n",
    "    \n",
    "    # check if the line is section declaration line < : capital-common-country >\n",
    "    # if yes >> take as section title\n",
    "    if line.startswith(':'):\n",
    "        \n",
    "        if i > 0:\n",
    "            analogy.append(current_analogy_set)\n",
    "        \n",
    "        current_section = line[2:]\n",
    "        current_analogy_set = {'section': current_section, 'pairs': []}\n",
    "    \n",
    "    # otherwise >> record as each word and append into a list called pairs for each section\n",
    "    else:\n",
    "        words = line.split()\n",
    "        current_analogy_set['pairs'].append(words)\n",
    "    \n",
    "    i+=1\n",
    "        \n",
    "    # if not line:  # Empty line indicates the end of an analogy set\n",
    "    #     analogy.append(current_analogy_set)\n",
    "\n",
    "# Display the first analogy set for illustration\n",
    "print(analogy[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corpus_capital_country = where section = 'capital country'\n",
    "for j in range(len(analogy)):\n",
    "    if analogy[j]['section'] == 'capital-common-countries':\n",
    "        corpus_capital_country = analogy[j]['pairs']\n",
    "    \n",
    "    if analogy[j]['section'] == 'gram7-past-tense':\n",
    "        corpus_past_tense = analogy[j]['pairs']\n",
    "        \n",
    "# corpus_capital_country\n",
    "# corpus_past_tense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) syntactic and semantic accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00 %\n"
     ]
    }
   ],
   "source": [
    "# Sementic\n",
    "accuracy = similarities (corpus_capital_country, Skipgram, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Syntactic\n",
    "accuracy = similarities (corpus_past_tense, Skipgram, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram (Neg) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Sementic\n",
    "accuracy = similarities (corpus_capital_country, SkipgramNeg, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Syntactic\n",
    "accuracy = similarities (corpus_past_tense, SkipgramNeg, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Sementic\n",
    "accuracy = similarities (corpus_capital_country, glove, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.00\n"
     ]
    }
   ],
   "source": [
    "# Syntactic\n",
    "accuracy = similarities (corpus_past_tense, glove, vocab)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogy\n",
    "def analogy(x1, x2, y1):\n",
    "    result = gensim.most_similar(positive=[y1,x2], negative = [x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_similarity (lines):\n",
    "    \n",
    "    correct_count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        words = line # ['dancing', 'danced', 'falling', 'fell']\n",
    "        \n",
    "        # check whether words are in gensim model or not\n",
    "        words_checked = [word.lower() if word.lower() in gensim else 'unknown' for word in words] \n",
    "        # print (words_checked)\n",
    "        \n",
    "        # result = gensim.most_similar(positive = [words_checked[2], words_checked[1]], negative = [words_checked[0]])\n",
    "        # word : number (queen : 0.7699)\n",
    "        result = analogy (words_checked[0], words_checked[1], words_checked[2])\n",
    "        \n",
    "        if result == words_checked[3]:\n",
    "            correct_count += 1  \n",
    "        \n",
    "    return (correct_count / len(lines)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 93.87\n"
     ]
    }
   ],
   "source": [
    "# sementic\n",
    "accuracy = gensim_similarity (corpus_capital_country)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 55.45\n"
     ]
    }
   ],
   "source": [
    "# syntatic\n",
    "accuracy = gensim_similarity (corpus_past_tense)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Use Similarity dataset and find the Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tiger\\tcat\\t7.35\\n', 'tiger\\ttiger\\t10.00\\n', 'plane\\tcar\\t5.77\\n', 'train\\tcar\\t6.31\\n', 'television\\tradio\\t6.77\\n', 'media\\tradio\\t7.42\\n', 'bread\\tbutter\\t6.19\\n', 'cucumber\\tpotato\\t5.92\\n', 'doctor\\tnurse\\t7.00\\n', 'professor\\tdoctor\\t6.62\\n', 'student\\tprofessor\\t6.81\\n', 'smart\\tstupid\\t5.81\\n', 'wood\\tforest\\t7.73\\n', 'money\\tcash\\t9.15\\n', 'king\\tqueen\\t8.58\\n', 'king\\trook\\t5.92\\n', 'bishop\\trabbi\\t6.69\\n', 'fuck\\tsex\\t9.44\\n', 'football\\tsoccer\\t9.03\\n', 'football\\tbasketball\\t6.81\\n', 'football\\ttennis\\t6.63\\n', 'Arafat\\tJackson\\t2.50\\n', 'physics\\tchemistry\\t7.35\\n', 'vodka\\tgin\\t8.46\\n', 'vodka\\tbrandy\\t8.13\\n', 'drink\\teat\\t6.87\\n', 'car\\tautomobile\\t8.94\\n', 'gem\\tjewel\\t8.96\\n', 'journey\\tvoyage\\t9.29\\n', 'boy\\tlad\\t8.83\\n', 'coast\\tshore\\t9.10\\n', 'asylum\\tmadhouse\\t8.87\\n', 'magician\\twizard\\t9.02\\n', 'midday\\tnoon\\t9.29\\n', 'furnace\\tstove\\t8.79\\n', 'food\\tfruit\\t7.52\\n', 'bird\\tcock\\t7.10\\n', 'bird\\tcrane\\t7.38\\n', 'food\\trooster\\t4.42\\n', 'money\\tdollar\\t8.42\\n', 'money\\tcurrency\\t9.04\\n', 'tiger\\tjaguar\\t8.00\\n', 'tiger\\tfeline\\t8.00\\n', 'tiger\\tcarnivore\\t7.08\\n', 'tiger\\tmammal\\t6.85\\n', 'tiger\\tanimal\\t7.00\\n', 'tiger\\torganism\\t4.77\\n', 'tiger\\tfauna\\t5.62\\n', 'psychology\\tpsychiatry\\t8.08\\n', 'psychology\\tscience\\t6.71\\n', 'psychology\\tdiscipline\\t5.58\\n', 'planet\\tstar\\t8.45\\n', 'planet\\tmoon\\t8.08\\n', 'planet\\tsun\\t8.02\\n', 'precedent\\texample\\t5.85\\n', 'precedent\\tantecedent\\t6.04\\n', 'cup\\ttableware\\t6.85\\n', 'cup\\tartifact\\t2.92\\n', 'cup\\tobject\\t3.69\\n', 'cup\\tentity\\t2.15\\n', 'jaguar\\tcat\\t7.42\\n', 'jaguar\\tcar\\t7.27\\n', 'mile\\tkilometer\\t8.66\\n', 'skin\\teye\\t6.22\\n', 'Japanese\\tAmerican\\t6.50\\n', 'century\\tyear\\t7.59\\n', 'announcement\\tnews\\t7.56\\n', 'doctor\\tpersonnel\\t5.00\\n', 'Harvard\\tYale\\t8.13\\n', 'hospital\\tinfrastructure\\t4.63\\n', 'life\\tdeath\\t7.88\\n', 'travel\\tactivity\\t5.00\\n', 'type\\tkind\\t8.97\\n', 'street\\tplace\\t6.44\\n', 'street\\tavenue\\t8.88\\n', 'street\\tblock\\t6.88\\n', 'cell\\tphone\\t7.81\\n', 'dividend\\tpayment\\t7.63\\n', 'calculation\\tcomputation\\t8.44\\n', 'profit\\tloss\\t7.63\\n', 'dollar\\tyen\\t7.78\\n', 'dollar\\tbuck\\t9.22\\n', 'phone\\tequipment\\t7.13\\n', 'liquid\\twater\\t7.89\\n', 'marathon\\tsprint\\t7.47\\n', 'seafood\\tfood\\t8.34\\n', 'seafood\\tlobster\\t8.70\\n', 'lobster\\tfood\\t7.81\\n', 'lobster\\twine\\t5.70\\n', 'championship\\ttournament\\t8.36\\n', 'man\\twoman\\t8.30\\n', 'man\\tgovernor\\t5.25\\n', 'murder\\tmanslaughter\\t8.53\\n', 'opera\\tperformance\\t6.88\\n', 'Mexico\\tBrazil\\t7.44\\n', 'glass\\tmetal\\t5.56\\n', 'aluminum\\tmetal\\t7.83\\n', 'rock\\tjazz\\t7.59\\n', 'museum\\ttheater\\t7.19\\n', 'shower\\tthunderstorm\\t6.31\\n', 'monk\\toracle\\t5.00\\n', 'cup\\tfood\\t5.00\\n', 'journal\\tassociation\\t4.97\\n', 'street\\tchildren\\t4.94\\n', 'car\\tflight\\t4.94\\n', 'space\\tchemistry\\t4.88\\n', 'situation\\tconclusion\\t4.81\\n', 'word\\tsimilarity\\t4.75\\n', 'peace\\tplan\\t4.75\\n', 'consumer\\tenergy\\t4.75\\n', 'ministry\\tculture\\t4.69\\n', 'smart\\tstudent\\t4.62\\n', 'investigation\\teffort\\t4.59\\n', 'image\\tsurface\\t4.56\\n', 'life\\tterm\\t4.50\\n', 'start\\tmatch\\t4.47\\n', 'computer\\tnews\\t4.47\\n', 'board\\trecommendation\\t4.47\\n', 'lad\\tbrother\\t4.46\\n', 'observation\\tarchitecture\\t4.38\\n', 'coast\\thill\\t4.38\\n', 'deployment\\tdeparture\\t4.25\\n', 'benchmark\\tindex\\t4.25\\n', 'attempt\\tpeace\\t4.25\\n', 'consumer\\tconfidence\\t4.13\\n', 'start\\tyear\\t4.06\\n', 'focus\\tlife\\t4.06\\n', 'development\\tissue\\t3.97\\n', 'theater\\thistory\\t3.91\\n', 'situation\\tisolation\\t3.88\\n', 'profit\\twarning\\t3.88\\n', 'media\\ttrading\\t3.88\\n', 'chance\\tcredibility\\t3.88\\n', 'precedent\\tinformation\\t3.85\\n', 'architecture\\tcentury\\t3.78\\n', 'population\\tdevelopment\\t3.75\\n', 'stock\\tlive\\t3.73\\n', 'peace\\tatmosphere\\t3.69\\n', 'morality\\tmarriage\\t3.69\\n', 'minority\\tpeace\\t3.69\\n', 'atmosphere\\tlandscape\\t3.69\\n', 'report\\tgain\\t3.63\\n', 'music\\tproject\\t3.63\\n', 'seven\\tseries\\t3.56\\n', 'experience\\tmusic\\t3.47\\n', 'school\\tcenter\\t3.44\\n', 'five\\tmonth\\t3.38\\n', 'announcement\\tproduction\\t3.38\\n', 'morality\\timportance\\t3.31\\n', 'money\\toperation\\t3.31\\n', 'delay\\tnews\\t3.31\\n', 'governor\\tinterview\\t3.25\\n', 'practice\\tinstitution\\t3.19\\n', 'century\\tnation\\t3.16\\n', 'coast\\tforest\\t3.15\\n', 'shore\\twoodland\\t3.08\\n', 'drink\\tcar\\t3.04\\n', 'president\\tmedal\\t3.00\\n', 'prejudice\\trecognition\\t3.00\\n', 'viewer\\tserial\\t2.97\\n', 'peace\\tinsurance\\t2.94\\n', 'Mars\\twater\\t2.94\\n', 'media\\tgain\\t2.88\\n', 'precedent\\tcognition\\t2.81\\n', 'announcement\\teffort\\t2.75\\n', 'line\\tinsurance\\t2.69\\n', 'crane\\timplement\\t2.69\\n', 'drink\\tmother\\t2.65\\n', 'opera\\tindustry\\t2.63\\n', 'volunteer\\tmotto\\t2.56\\n', 'listing\\tproximity\\t2.56\\n', 'precedent\\tcollection\\t2.50\\n', 'cup\\tarticle\\t2.40\\n', 'sign\\trecess\\t2.38\\n', 'problem\\tairport\\t2.38\\n', 'reason\\thypertension\\t2.31\\n', 'direction\\tcombination\\t2.25\\n', 'Wednesday\\tnews\\t2.22\\n', 'glass\\tmagician\\t2.08\\n', 'cemetery\\twoodland\\t2.08\\n', 'possibility\\tgirl\\t1.94\\n', 'cup\\tsubstance\\t1.92\\n', 'forest\\tgraveyard\\t1.85\\n', 'stock\\tegg\\t1.81\\n', 'month\\thotel\\t1.81\\n', 'energy\\tsecretary\\t1.81\\n', 'precedent\\tgroup\\t1.77\\n', 'production\\thike\\t1.75\\n', 'stock\\tphone\\t1.62\\n', 'holy\\tsex\\t1.62\\n', 'stock\\tCD\\t1.31\\n', 'drink\\tear\\t1.31\\n', 'delay\\tracism\\t1.19\\n', 'stock\\tlife\\t0.92\\n', 'stock\\tjaguar\\t0.92\\n', 'monk\\tslave\\t0.92\\n', 'lad\\twizard\\t0.92\\n', 'sugar\\tapproach\\t0.88\\n', 'rooster\\tvoyage\\t0.62\\n', 'noon\\tstring\\t0.54\\n', 'chord\\tsmile\\t0.54\\n', 'professor\\tcucumber\\t0.31\\n', 'king\\tcabbage\\t0.23\\n']\n"
     ]
    }
   ],
   "source": [
    "# load the txt file\n",
    "wordsim_filepath = '../A1 - Search Engine/wordsim_similarity_goldstandard.txt'\n",
    "\n",
    "with open(wordsim_filepath, 'r') as file:\n",
    "    wordsim = file.readlines()\n",
    "    print(wordsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine_similarity (A,B):\n",
    "    dot_product = np.dot(A.flatten(),B.flatten())\n",
    "    norm_a      = np.linalg.norm(A)\n",
    "    nomr_b      = np.linalg.norm(B)\n",
    "    similarity  = dot_product / (norm_a * nomr_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate correlation\n",
    "\n",
    "def correlation_func (txt, model, vocab):\n",
    "    predicted_scores = []\n",
    "    actual_scores    = []\n",
    "    # cos_sim_value = 0\n",
    "    \n",
    "    for line in txt:\n",
    "        \n",
    "        # Clean and tokenize the line\n",
    "        line = line.strip().split() # ['cat','tiger','7.3']\n",
    "        \n",
    "        # Get embeddings for each word in the line, or use '<UNK>' if not in vocab\n",
    "        word_checked = [model.get_embed(word) if word in vocab else model.get_embed('<UNK>') for word in line[:-1]]\n",
    "        # print(word_checked[0])\n",
    "        \n",
    "        # cal cosine similarity\n",
    "        cosine_sim_value = cosine_similarity(word_checked[0].detach().numpy(), word_checked[1].detach().numpy())\n",
    "        \n",
    "        predicted_scores.append(cosine_sim_value)\n",
    "        actual_scores.append(float(line[-1]))\n",
    "        \n",
    "        result = spearmanr(predicted_scores, actual_scores)\n",
    "        \n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store all the model and correlation result\n",
    "correlation_result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram correlation score: 0.09390973994656401\n"
     ]
    }
   ],
   "source": [
    "print(f\"Skipgram correlation score: {correlation_func(wordsim, Skipgram, vocab)}\")\n",
    "skipgramCorrelation = correlation_func(wordsim, Skipgram, vocab)\n",
    "correlation_result['skipgram'] = skipgramCorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Skipgram (Negative Sampling) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegSkipgram correlation score: 0.08443987589946488\n"
     ]
    }
   ],
   "source": [
    "print(f\"NegSkipgram correlation score: {correlation_func(wordsim, SkipgramNeg, vocab)}\")\n",
    "negCorrelation = correlation_func(wordsim, SkipgramNeg, vocab)\n",
    "correlation_result['Neg'] = negCorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe correlation score: 0.0844819200165613\n"
     ]
    }
   ],
   "source": [
    "print(f\"GloVe correlation score: {correlation_func(wordsim, glove, vocab)}\")\n",
    "gloveCorrelation = correlation_func(wordsim, glove, vocab)\n",
    "correlation_result['glove'] = gloveCorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gensim  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_dist(txt):\n",
    "\n",
    "    gen_dist = []\n",
    "    actual_dist = []\n",
    "\n",
    "    for line in txt:\n",
    "        \n",
    "        # Clean and tokenize the line\n",
    "            line = line.strip().split() # ['cat','tiger','7.3']\n",
    "            \n",
    "            # Get embeddings for each word in the line, or use '<UNK>' if not in vocab\n",
    "            word_checked = [word.lower() if word.lower() in gensim else 'unknown' for word in line[:-1]]\n",
    "            \n",
    "            dist = gensim.distance(word_checked[0], word_checked[1])\n",
    "            \n",
    "            gen_dist.append(dist)\n",
    "            actual_dist.append(float(line[-1]))\n",
    "            \n",
    "    return spearmanr(gen_dist, actual_dist)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6035208452876207\n"
     ]
    }
   ],
   "source": [
    "gensimCorrelation = gensim_dist(wordsim)\n",
    "print(f\"{gensimCorrelation}\")\n",
    "correlation_result['Gensim'] = gensimCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipgram correlation: 0.09390973994656401\n",
      "Neg correlation: 0.08443987589946488\n",
      "glove correlation: 0.0844819200165613\n",
      "Gensim correlation: -0.6035208452876207\n"
     ]
    }
   ],
   "source": [
    "for model, correlationS in correlation_result.items():\n",
    "    print(f\"{model} correlation: {correlationS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Similarity Metrics (Correlation between my models and human judgement)\n",
    "\n",
    "Model             | Skipgram | NEG | GloVe | GloVe (gensim) | Y-true |\n",
    "|----------------- |---------|-----|-------|----------------|--------|\n",
    "| Spearmanr Result | 0.0939  | 0.0844 | 0.0845  | -0.6035   |   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
