{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1 : Search Engine (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.25.2', '2.1.0', '3.7.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the nltk dataset = rural\n",
    "txt_file = './abc/rural.txt'\n",
    "\n",
    "with open(txt_file, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "# Split the dataset into paragraphs based on double line breaks : to get one paragraph in a list item ['paragraph1', 'paragraph']\n",
    "paragraphs = [paragraph.strip() for paragraph in text.split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(paragraphs)):\n",
    "    # Replace newline characters with spaces\n",
    "    paragraphs[i] = paragraphs[i].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. tokenization\n",
    "corpus = [sent.split(\" \") for sent in paragraphs]\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the corpus only accept 100 documents\n",
    "corpus = corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique word\n",
    "\n",
    "# list comprehension for getting words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# getting unique word and store as a list\n",
    "vocab = list(set(flatten(corpus)))\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <UNK> to a dictionary vocab\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization: assign index to each word\n",
    "word2index = {w:idx for idx, w in enumerate(vocab)}\n",
    "# word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index2word\n",
    "index2word = {k:v for v,k in word2index.items()}\n",
    "# index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation for the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "\n",
    "    # define a list for storing [center,outside] pair\n",
    "    skipgrams = []\n",
    "\n",
    "    # loop each word sequence\n",
    "    for sent in corpus:\n",
    "        \n",
    "        for i in range(2, len(sent)-2):\n",
    "            \n",
    "            # assign center word\n",
    "            center_word = word2index[sent[i]]\n",
    "            \n",
    "            # assign outside word=4 (ws = 2)\n",
    "            outside_word = [word2index[sent[i-2]], word2index[sent[i-1]], word2index[sent[i+1]], word2index[sent[i+2]]]\n",
    "            \n",
    "            # for each of these two outside words, we gonna pair (center,outside) and append to a list\n",
    "            for each_outside in outside_word:\n",
    "                skipgrams.append([center_word, each_outside])\n",
    "                \n",
    "    # randomly select 2 pair among the data\n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace = False)\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skipgrams[i][0]]) # center_word\n",
    "        random_labels.append([skipgrams[i][1]]) # outside_word\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Negative Sampling\n",
    "\n",
    "### Unigram Distribution\n",
    "\n",
    "$$P(w) = U(w)^{3/4} / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0.001 # according to the papaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14716\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "word_count # {'apple': 10, 'orange' : 5}\n",
    "\n",
    "# get the total number of words\n",
    "num_total_words = sum([c for w,c in word_count.items()])\n",
    "print(num_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U(w)\n",
    "\n",
    "unigram_table = []\n",
    "\n",
    "for v in vocab:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    # print(v, uw)\n",
    "    # print(v, uw_alpha)\n",
    "    # print([v] * uw_alpha)\n",
    "    # print(\"---\")\n",
    "    unigram_table.extend([v] * uw_alpha) \n",
    "    \n",
    "# Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence (seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index['<UNK>'],seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    \n",
    "    batch_size  = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    \n",
    "    for i in range(batch_size): # (1,k) # (batch_size, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1,-1))\n",
    "            \n",
    "        \n",
    "    return torch.cat(neg_samples) # [batch_size,k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x,y = random_batch(batch_size, corpus)\n",
    "x_tensor = torch.LongTensor(x)\n",
    "y_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "neg_samples = negative_sampling(y_tensor, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "tensor([3738,  368, 1626, 3698, 4119])\n",
      "tensor([1181, 3841, 1208, 2747, 2226])\n",
      "[[ 936]\n",
      " [1414]]\n"
     ]
    }
   ],
   "source": [
    "print(neg_samples.shape)\n",
    "print(neg_samples[0])\n",
    "print(neg_samples[1])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg (nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative_words):\n",
    "        \n",
    "        # center, outside : (bs, 1)\n",
    "        # negative : (bs, k)\n",
    "        \n",
    "        center_embed  = self.embedding_center(center) # (bs, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        neg_embed     = self.embedding_outside(negative_words) # (bs, k, emb_size)\n",
    "        \n",
    "        uovc          = outside_embed.bmm(center_embed.transpose(1,2)).squeeze(2) # (bs,1)\n",
    "        ukvc          = -neg_embed.bmm(center_embed.transpose(1,2)).squeeze(2) #(bs,k)\n",
    "        ukvc_sum      = torch.sum(ukvc, 1).reshape(-1,1) # sum across k , reshape>> (batch_size,1)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocab)\n",
    "model = SkipgramNeg(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3738,  368, 1626, 3698, 4119],\n",
       "        [1181, 3841, 1208, 2747, 2226]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_tensor, y_tensor, neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1676, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# for recording the training time for each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time # get the total taken timestamp\n",
    "    elapsed_mins = int(elapsed_time / 60) # get the min\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) # get the sec\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "emb_size = 2\n",
    "voc_size = len(vocab)\n",
    "model    = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss 1.647907\n",
      "Epoch   2000 | Loss 0.489159\n",
      "Epoch   3000 | Loss 4.145045\n",
      "Epoch   4000 | Loss 3.187397\n",
      "Epoch   5000 | Loss 0.798258\n"
     ]
    }
   ],
   "source": [
    "# num_epochs = 5\n",
    "num_epochs = 5000\n",
    "loss_ar = []\n",
    "starttime = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k)\n",
    "    loss = model (input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    # backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    # record loss\n",
    "    loss_ar.append(loss)\n",
    "    \n",
    "    # print loss\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss {loss:2.6f}\")\n",
    "\n",
    "endtime = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_training_time: 3m : 1s\n"
     ]
    }
   ],
   "source": [
    "total_training_time = epoch_time(starttime,endtime)\n",
    "print(f\"total_training_time: {total_training_time[0]}m : {total_training_time[1]}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7983, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed (word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), './model/A1-NegSampling.pt')\n",
    "\n",
    "# save the data\n",
    "Data = {\n",
    "    'corpus': corpus,\n",
    "    'vocab': vocab,\n",
    "    'word2index': word2index,\n",
    "    'voc_size': voc_size,\n",
    "    'embedding_size': emb_size\n",
    "}\n",
    "\n",
    "pickle.dump(Data,open('./model/Data.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
