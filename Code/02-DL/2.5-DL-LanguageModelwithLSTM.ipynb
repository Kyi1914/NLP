{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Models\n",
    "\n",
    "this will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.\n",
    "\n",
    "Paper that we base on is Regularizing and Optimizing LSTM Language Models. https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm # progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 # to generate the same results\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task. This time, we will use the datasets library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from wiki\n",
    "datasets = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the text data inside the dataset.\n",
    "# print(dataset['which dataset'][which row]['which features'])\n",
    "print(datasets['train'][223]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets['train'].shape) # (row, col) (36718 rows of text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "# create a function\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# def tokenize_data(example, tokenizer):\n",
    "#     tokens = tokenizer(example['text'])\n",
    "#     return tokens\n",
    "\n",
    "tokenized_dataset = datasets.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset['train'][223]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big. Also we shall make sure to add unk and eos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator (tokenized_dataset['train']['tokens'], min_freq = 3)\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data (with 4 words)\n",
    "- \"Chaky loves eating at\", \n",
    "- \"AIT <eos> I really\", \n",
    "- \"love deep learning <eos>\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset: # get >> example = Chaky loves eating at AIT\n",
    "        if example['tokens']:\n",
    "            \n",
    "            # add '<eos>' at the end of each sentence, (example) in this case, inside dataset \n",
    "            # ['Chaky', 'loves', 'eating', 'at', 'AIT', '<eos>']\n",
    "            tokens = example['tokens'].append('<eos>') \n",
    "            \n",
    "            # apply numericalization\n",
    "            tokens = [vocab[token] for token in example['tokens']] # [6,2,3,5,1]\n",
    "            data.extend(tokens)\n",
    "    \n",
    "    # convert data type to torch for embedding        \n",
    "    data = torch.LongTensor(data)\n",
    "    \n",
    "    num_batches = data.shape[0] // batch_size # // is integer division 4\n",
    "    \n",
    "    # just to make sure all batches are even\n",
    "    data = data[:num_batches * batch_size] # to update the data size that is divisable by batch_size, consider odd number for data.shape[0]\n",
    "    \n",
    "    data = data.view(batch_size, num_batches) # (3,4) # view vs. reshape (whether data is contiguous)\n",
    "    \n",
    "    return data # [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape # each train total 128 batch and each batch contain 16214 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers = num_layers, \n",
    "                                  dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb   = 0.1\n",
    "        init_range_other = 1 / math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(\n",
    "                -init_range_other, init_range_other) # We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, self.hid_dim).uniform_(\n",
    "                -init_range_other, init_range_other) # Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden # return from LSTM that is tuple that contains hidden and cell values\n",
    "        hidden       = hidden.detach() # not to be used for gradient computation\n",
    "        cell         = cell.detach()\n",
    "        return hidden, cell\n",
    "    \n",
    "    def forward(self, src, hidden):\n",
    "        \n",
    "        # src: [batch_size, seq_len]\n",
    "        \n",
    "        embedding      = self.dropout(self.embedding(src)) # src = harry potter is \n",
    "        # embedding: [batch-size, seq_len, emb_dim]\n",
    "        \n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        # output: [batch_size, seq_len, emb_dim]\n",
    "        #hidden: [num_layers * direction, seq_len, hid_dim]\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        prediction = self.fc(output)\n",
    "        # predcition: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024 # 400 in the paper\n",
    "hid_dim = 1024 # 1150 in the paper\n",
    "num_layers = 2 # 3 in the paper\n",
    "dropout_rate = 0.65\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'the model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch (data, seq_len, idx):\n",
    "    # data #[batch_size, bunch of tokens]\n",
    "    src    = data[:, idx  : idx+seq_len]\n",
    "    target = data[:, idx+1: idx+seq_len+1] # target simply is ahead of src by 1\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch_size, seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data        = data[:, :num_batches - (num_batches - 1) % seq_len] # we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    # reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches-1, seq_len), desc = 'Training: ', leave = False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) # src, target: [batch_size, seq_len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size  = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)      \n",
    "        \n",
    "        # need to reshape because criterion expects pred to be 2d and target to be 1d \n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target     = target.reshape(-1)\n",
    "        loss       = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model, data, criterion, batch_size, seq_len, device):\n",
    "    \n",
    "    epoh_loss = 0\n",
    "    model.eval() # evaluation\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden      = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size  = src.shape[0]\n",
    "            \n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape (batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "            \n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss +=loss.item() * seq_len\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 # decoding length\n",
    "clip = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau (optimizer, factor = 0.5, patience = 0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate (model, valid_data, criterion, batch_size, seq_len, device)\n",
    "    \n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "        \n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt', map_location = device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   7. Real-worl inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed = None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
