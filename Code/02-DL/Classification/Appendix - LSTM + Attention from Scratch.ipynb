{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - LSTM + Attention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch \n",
    "from torch import nn \n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vanilla LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim:int, hidden_dim:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # initialise the trianable Parameters\n",
    "        # for input gate\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        # for forget gate\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        # for input (tanh layer, updated state)\n",
    "        self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        # for output gate\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "            \n",
    "        # heuristic for weight initialization.\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            # initialize its values from a uniform distribution with lower bound -stdv and upper bound stdv\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        '''\n",
    "        This initialization method is known as \"Xavier\" or \"Glorot\" initialization, \n",
    "        and it helps in preventing the vanishing/exploding gradient problem during training. \n",
    "        It's commonly used for weights in neural networks, \n",
    "        especially in the context of tanh or sigmoid activation functions.\n",
    "        '''    \n",
    "        \n",
    "    def forward(self, x, init_states = None):\n",
    "        \"\"\"\n",
    "        x.shape = (bs, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        bs, seq_len, _ = x.shape\n",
    "        \n",
    "        # to store the hidden list\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:          \n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        # for each time step of the input x, calculate LSTM gates (f_t, i_t, o_t, g_t) \n",
    "        # and updates the cell state (c_t) and hidden state (h_t) accordingly.\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] #get x data of time step t (SHAPE: (batch_size, input_dim))\n",
    "            \n",
    "            f_t = torch.sigmoid(h_t @ self.W_f + x_t @ self.U_f + self.b_f)\n",
    "            i_t = torch.sigmoid(h_t @ self.W_i + x_t @ self.U_i + self.b_i)\n",
    "            o_t = torch.sigmoid(h_t @ self.W_o + x_t @ self.U_o + self.b_o)\n",
    "            g_t = torch.sigmoid(h_t @ self.W_g + x_t @ self.U_g + self.b_g)\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0))\n",
    "            # reshape h_t to (1, batch_size, hidden_dim), then append to the list of hidden\n",
    "        \n",
    "        # The list is then concatenated to form a tensor of shape (seq_len, batch_size, hidden_dim).\n",
    "        output = torch.cat(output, dim = 0) # concatenate h_t of all time steps into SHAPE: (seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0,1).contiguous() # just transpose to SHAPE: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        return output, (h_t, c_t)\n",
    "        # return ouput (hidden state tensor), final hidden state and final cell state\n",
    "        \n",
    "        # This forward pass is one step of an LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = 5000\n",
    "hidden_dim = 256\n",
    "embed_dim  = 300\n",
    "output_dim = 1\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking LSTM cell can run\n",
    "\n",
    "my_LSTM_cell = LSTM_cell(embed_dim, hidden_dim).to(device)\n",
    "output, (h_t, c_t) = my_LSTM_cell(test data)\n",
    "\n",
    "assert output.shape == torch.Size([32, 100, 256])\n",
    "assert h_t.shape == torch.Size([32, 256])\n",
    "assert c_t.shape == torch.Size ([32, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla / Peephole / Coupled LSTM from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, lstm_type: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_type = lstm_type\n",
    "        \n",
    "        # initialise the trainable Parameters\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        if self.lstm_type == 'peephole' :\n",
    "            self.P_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] # get x data of time step t (SHAPE: (batch_size, input_dim))\n",
    "            \n",
    "            if self.lstm_type in ['vanilla', 'coupled'] :\n",
    "                f_t = torch.sigmoid(    h_t @ self.W_f  +  x_t @ self.U_f  +  self.b_f)\n",
    "                o_t = torch.sigmoid(    h_t @ self.W_o  +  x_t @ self.U_o  +  self.b_o)\n",
    "                if self.lstm_type == 'vanilla':\n",
    "                    i_t = torch.sigmoid(    h_t @ self.W_i  +  x_t @ self.U_i  +  self.b_i)\n",
    "                if self.lstm_type == 'coupled':\n",
    "                    i_t = (1 - f_t)\n",
    "            if self.lstm_type == 'peephole' :\n",
    "                i_t = torch.sigmoid( h_t @ self.W_i + x_t @ self.U_i + c_t @ self.P_i + self.b_i) # SHAPE: (batch_size, hidden_dim)\n",
    "                f_t = torch.sigmoid( h_t @ self.W_f + x_t @ self.U_f + c_t @ self.P_f + self.b_f) # SHAPE: (batch_size, hidden_dim)\n",
    "                o_t = torch.sigmoid( h_t @ self.W_o + x_t @ self.U_o + c_t @ self.P_o + self.b_o) # SHAPE: (batch_size, hidden_dim)\n",
    "            \n",
    "            g_t = torch.tanh(       h_t @ self.W_g  +  x_t @ self.U_g   + self.b_g)\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then append to the list of hidden states\n",
    "\n",
    "        output = torch.cat(output, dim = 0) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1).contiguous() # just transpose to SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biLSTM with vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, input_dim:int, embed_dim:int, hidden_dim:int, output_dim:int):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2\n",
    "        \n",
    "        self.embedding      = nn.Embedding(input_dim, embed_dim, padding_idx = pad_idx)\n",
    "        self.hidden_dim     = hidden_dim\n",
    "        \n",
    "        # Define forward and backward LSTM cells\n",
    "        self.forward_lstm   = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        self.backward_lstm  = new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        \n",
    "        # Learnable weights for combining the forward and backward hidden states\n",
    "        self.W_h = nn.Parameter(torch.Tensor(hidden_dim * self.num_directions, hidden_dim * self.num_directions))\n",
    "        self.b_h = nn.Parameter(torch.Tensor(hidden_dim * self.num_directions))\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc  = nn.Linear(hidden_dim * self.num_directions, output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "    \n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, text, text_length):\n",
    "        \n",
    "        # Embedding Layer\n",
    "        embedded = self.embedding(text)\n",
    "        embedded_flip = torch.flip(embedded, [1])\n",
    "        \n",
    "        # Forward and backward LSTM pass\n",
    "        output_forward, (hn_forward, cn_forward)    = self.forward_lstm (embedded, init_states = None)\n",
    "        output_backward, (hn_backward, cn_backward) = self.backward_lstm(embedded_flip, init_states = None)\n",
    "        \n",
    "        # Concatenate the hidden states from the forward and backward LSTMs\n",
    "        # concat the hidden state at the last word of the sentence and hidden state at the first sentence\n",
    "        # hidden state at the last word = hs from forward lstm \n",
    "        # hidden state at the first word = hs from the backward lstm\n",
    "        concat_hn = torch.cat((hn_forward, hn_backward), dim = 1)\n",
    "        \n",
    "        # Apply a linear transformation to combine the hidden states\n",
    "        ht = torch.sigmoid(concat_hn @ self.W_h + self.b_n)\n",
    "        \n",
    "        return self.fc(ht) # Pass ht to another linear layer to get the output for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + General Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_GAtt(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim:int, embed_dim:int, hidden_dim:int, output_dim:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # Use pytorch LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, dropout = dropout, batch_first = True )\n",
    "        \n",
    "        # linear layer for binary classification\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def attention_net(self, lstm_output, hn):\n",
    "        h_t      = hn.unsqueeze(2)\n",
    "        H_keys   = torch.clone(lstm_output)\n",
    "        H_values = torch.clone(lstm_output)\n",
    "        \n",
    "        alignment_score = torch.bmm(H_keys, h_t).squeeze(2) # shape : (bs, seq_len, 1)\n",
    "        \n",
    "        soft_attn_weights = F.softmax(alignment_score, 1) # shape : (bs, seq_len, 1)\n",
    "        \n",
    "        context = torch.bmm(H_values.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2) # shape : (bs, hid_dim)\n",
    "        \n",
    "        return context\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        embedded = self.embedding(text) # shape (bs, seq_len, emb_dim)\n",
    "        \n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # this is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        attn_output = self.attention_net(lstm_output, hn)\n",
    "        \n",
    "        return self.fc(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This attention mask will be apply after Q @ K^T thus the shape will be batch, seq_len, seq_len\n",
    "def get_pad_mask(text):  #[batch, seq_len]\n",
    "    batch_size, seq_len = text.size()\n",
    "    # eq(zero) is lstm output over PAD token\n",
    "    pad_mask = text.data.eq(0).unsqueeze(1)  # torch.eq Computes element-wise equality # batch_size x 1 x seq_len; we unsqueeze so we can make expansion below\n",
    "    return pad_mask.expand(batch_size, seq_len, seq_len)  # batch_size x seq_len x seq_len\n",
    "\n",
    "class LSTM_SelfAtt(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, output_dim, len_reduction):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Long Softmax Layer for Classification\n",
    "        self.softmax       = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # initialize three linear layers for Q, K, V\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "        self.len_reduction = len_reduction\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc    = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def self_attention_net(self, lstm_output, pad_mask):\n",
    "        \n",
    "        # create three copies of output (H)\n",
    "        Q = self.lin_Q(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "        K = self.lin_K(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "        V = self.lin_V(torch.clone(lstm_output)) # SHAPE : (bs, seq_len, n_hidden * num_directions)\n",
    "        \n",
    "        # attention score        \n",
    "        alignment_score = torch.matmul(Q, K.transpose(1, 2)) # SHAPE : (bs, seq_len, seq_len)\n",
    "                \n",
    "        # Apply padding mask \n",
    "        if self.mask:\n",
    "            alignment_score.masked_fill_(pad_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        soft_attn_weights = self.softmax(alignment_score)\n",
    "        \n",
    "        # Weighted sum to get context\n",
    "        context = torch.matmul(soft_attn_weights, V) # SHAPE : (bs, seq_len, hidden_dim * num_directions)\n",
    "        \n",
    "        # Length reduction options: mean, sum, last\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1)\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1)\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :]\n",
    "        \n",
    "    def forward(self, text, text_lengths, mask=True):\n",
    "        self.mask = mask\n",
    "        pad_mask = get_pad_mask(text)\n",
    "        \n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenating the forward and backward hidden states\n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_output = self.self_attention_net(lstm_output, pad_mask)\n",
    "        \n",
    "        return self.fc(attn_output) # Classification using a linear layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
