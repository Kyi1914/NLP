{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "with open(\"./data/wiki_king.txt\", \"r\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[97, 85, 11, 30, 49, 69, 11, 74, 28, 68, 22, 25, 6, 44, 15, 62, 91, 88, 60, 81, 51, 99, 21, 46, 40, 18, 29, 34, 51, 32, 94, 40, 88, 35, 27, 7, 86, 41, 72, 96, 103, 94, 70], [11, 36, 35, 27, 23, 57, 51, 35, 16, 12, 17, 96, 39, 67, 51, 40, 24, 4, 71, 92, 82], [98, 62, 40, 101, 71, 54, 35, 27, 99, 51, 81, 97, 85, 11, 90, 50, 94, 51, 52, 105, 107, 8, 45, 75, 63, 33, 65, 27, 51, 78, 83, 35, 79], [11, 52, 40, 41, 20, 87, 84, 51, 38, 46, 40, 43, 47, 71, 40, 76, 39, 80, 37, 95], [10, 47, 71, 40, 48, 98, 52, 40, 77, 66, 7, 94, 66, 93, 51, 55, 26, 9, 71, 40, 24, 64, 14, 94, 100, 59], [11, 55, 26, 40, 64, 56, 19, 73, 5, 98, 106, 96, 53, 102, 89, 13, 58, 42, 19, 40, 31, 71, 40, 104, 61]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Assuming word2id is defined somewhere in your code\n",
    "# word2id = {'example': 1, ...}\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sentence.text.lower() for sentence in doc.sents]  # Convert sentences to lowercase\n",
    "sentences = [re.sub(\"[.,!?\\\\-]\", '', sentence) for sentence in sentences]  # Clean symbols from each sentence\n",
    "\n",
    "#making vocabs - numericalization\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n",
    "    id2word    = {i:w for i, w  in enumerate(word2id)}\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "token_list = []\n",
    "for sentence in sentences:\n",
    "    arr = [word2id.get(word, 0) for word in sentence.split()]  # Use .get() to avoid KeyError for unknown words\n",
    "    token_list.append(arr)\n",
    "\n",
    "print(token_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_a:[11, 52, 40, 41, 20, 87, 84, 51, 38, 46, 40, 43, 47, 71, 40, 76, 39, 80, 37, 95]\n",
      "tokens_b:[98, 62, 40, 101, 71, 54, 35, 27, 99, 51, 81, 97, 85, 11, 90, 50, 94, 51, 52, 105, 107, 8, 45, 75, 63, 33, 65, 27, 51, 78, 83, 35, 79]\n",
      "input_ids:[1, 11, 52, 40, 41, 20, 87, 84, 51, 38, 46, 40, 43, 47, 71, 40, 76, 39, 80, 37, 95, 2, 98, 62, 40, 101, 71, 54, 35, 27, 99, 51, 81, 97, 85, 11, 90, 50, 94, 51, 52, 105, 107, 8, 45, 75, 63, 33, 65, 27, 51, 78, 83, 35, 79, 2]\n",
      "segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#randomly choose two sentence\n",
    "tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "print(f\"tokens_a:{tokens_a}\")\n",
    "print(f\"tokens_b:{tokens_b}\")\n",
    "        \n",
    "#1. token embedding - add CLS and SEP\n",
    "input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "print(f\"input_ids:{input_ids}\")\n",
    "\n",
    "#2. segment embedding - which sentence is 0 and 1\n",
    "segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "print(f\"segment_ids:{segment_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n",
    "max_len    = 1000 #maximum length that my transformer will accept.....all sentence will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        \n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "        \n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        #3 masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        \n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "        \n",
    "    return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "batch = make_batch()\n",
    "print(len(batch))\n",
    "\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1000]),\n",
       " torch.Size([6, 1000]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(input_ids[0])\n",
    "# input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 89,  66,  98,  31,  40],\n",
       "        [ 82,  11,  40,  35,  51],\n",
       "        [ 88,  41,  49,  30,  51],\n",
       "        [ 67, 101,  65,  52,  11],\n",
       "        [ 59,  61,  40,  13,  66],\n",
       "        [ 40,  51,  98,  71,  94]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15, 39, 32, 21, 23],\n",
       "        [42, 22, 11, 29, 28],\n",
       "        [33, 38, 49,  4, 21],\n",
       "        [14, 26, 49, 41, 36],\n",
       "        [26, 52,  4, 43, 10],\n",
       "        [41,  8, 27, 14, 33]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that BERT only uses the encoder.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "- Embedding layers\n",
    "- Attention Mask\n",
    "- Encoder layer\n",
    "- Multi-head attention\n",
    "- Scaled dot product attention\n",
    "- Position-wise feed-forward network\n",
    "- BERT (assembling all the components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq_q = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 0]])\n",
    "# seq_k = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 0]])\n",
    "# pad_mask = get_attn_pad_mask(seq_q, seq_k)\n",
    "# pad_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network\n",
    "\n",
    "First let's make the wrapper called `EncoderLayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
