{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2 : Summarization with Seq2Seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678944384658,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "3Vo07AlhJ1Tp"
   },
   "outputs": [],
   "source": [
    "enter_name = \"Kyi Thin Nu\"\n",
    "student_id = \"st124087\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library & Dataset (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3608,
     "status": "ok",
     "timestamp": 1678944388263,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "Wt86c1YhJ1Tr",
    "outputId": "63d251d6-ea25-4745-d83b-d068475263dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.1.1\n",
    "# !pip install torchtext==0.16.1\n",
    "# !pip install datasets\n",
    "# !pip install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 1969,
     "status": "ok",
     "timestamp": 1678944390230,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-SgD_bU1J1Ts"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "aeeb7e2754454060bfe83a5908d94600",
      "c1573faf199443d192e2d5f996054616",
      "025113dc59724811bfb88724bb8edd43",
      "b45dc527e44c45808d627a7382055aae",
      "dbc27640827b498d9de76829b0ca911c",
      "bafd0076a9344677b4d21e66153c1394",
      "035556ace0094a1db07bb17e0fd170ce",
      "2d970695aff445b29ef9402fc8b64936",
      "14578b0e69b9495ab871767777b0d790",
      "b7e910835d584f69b72443d20b662e1b",
      "4243f7f3c9f54b66b659911728d5316f"
     ]
    },
    "executionInfo": {
     "elapsed": 3983,
     "status": "ok",
     "timestamp": 1678944394210,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "EkuKq3LVJ1Tt",
    "outputId": "571b05b7-a7e9-4546-8f8e-1fc9ba4cb458"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load CNN/DM dataset from huggingface\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944394211,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "rxEyYJcmJ1Tt"
   },
   "outputs": [],
   "source": [
    "## Load the train data and the validation data\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678944394211,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "vugXbqanNU_K",
    "outputId": "8548384a-4572-4770-c70f-224db08641ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article', 'highlights']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(train_data.features.keys())\n",
    "columns.pop()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944395006,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "sSWXLQ5oNyQh",
    "outputId": "f411e5ae-9569-4d8d-ef1d-650e0e709e5b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.',\n",
       " \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "train_data[0]['article'], train_data[0]['highlights']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wTXuiyV_J1Tu"
   },
   "source": [
    "## Preprocessing (2 points)\n",
    "- Convert the dataset into pandas DataFrame with proper column names\n",
    "    - The training dataset is too big use only 1000 sample maximum\n",
    "    - The validation dataset is too big use only 200 sample maximum\n",
    "- lowercase everything\n",
    "- remove unnecessary words that would not make sense\n",
    "- remove linebreaks and backslashes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 12090,
     "status": "ok",
     "timestamp": 1678944407094,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "FOzGtHaQJ1Tu"
   },
   "outputs": [],
   "source": [
    "###Convert the dataset into pandas DataFrame with proper column names\n",
    "df_train =  [[i['article'], i['highlights']] for i in train_data]\n",
    "df_valid = [[i['article'], i['highlights']] for i in valid_data]\n",
    "train_df = pd.DataFrame(df_train, columns = columns)\n",
    "valid_df = pd.DataFrame(df_valid, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1678944407094,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "2UcIIa0ZJ1Tu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (200, 2))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The training dataset is too big use only 1000 sample maximum\n",
    "### The validation dataset is too big use only 200 sample maximum\n",
    "#code here\n",
    "train_df = train_df[0:1000]\n",
    "valid_df = valid_df[0:200]\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-wciN_rKJ1Tv"
   },
   "outputs": [],
   "source": [
    "# Applying Lower Case to the text\n",
    "#code here\n",
    "#hint : using apply\n",
    "\n",
    "#traing dataset\n",
    "train_df['article']    =  train_df['article'].apply(str.lower)\n",
    "train_df['highlights'] =  train_df['highlights'].apply(str.lower)\n",
    "\n",
    "#validation dataset\n",
    "valid_df['article']    =  valid_df['article'].apply(str.lower)\n",
    "valid_df['highlights'] =  valid_df['highlights'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "ZJG9j0WKRUEA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'has do the right thang? he has a new show'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Removing unnesscary words \n",
    "###Using Regex\n",
    "def data_cleaning(data):\n",
    "    regex_s = re.sub(\"\\\\(.+?\\\\)|[\\r\\n|\\n\\r]|-\", \"\", data)\n",
    "    fin     = \" \".join(regex_s.split())\n",
    "    return fin\n",
    "\n",
    "asd = \"(cnn) has do the right thang? \\n he has -- a new show \"\n",
    "data_cleaning(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1678944407095,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "oJaMNrcsTQVF"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "#using above function to apply removing unnesscary words\n",
    "#hint : using apply\n",
    "train_df['article']    = train_df['article'].apply(data_cleaning)\n",
    "train_df['highlights'] = train_df['highlights'].apply(data_cleaning)\n",
    "valid_df['article']    = valid_df['article'].apply(data_cleaning)\n",
    "valid_df['highlights'] = valid_df['highlights'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 8280,
     "status": "ok",
     "timestamp": 1678944415345,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "nyjA2FIRJ1Tv"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678944415346,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "DL93MVtLJ1Tv"
   },
   "outputs": [],
   "source": [
    "def yield_tokens(data):\n",
    "    #complete this code\n",
    "    # for _, text in data:\n",
    "    for text in data:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678944415346,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "FwNKd8nRJ1Tv"
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 3339,
     "status": "ok",
     "timestamp": 1678944418675,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "Ppnf2DquJ1Tw"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "#code here\n",
    "vocab_transform = build_vocab_from_iterator(yield_tokens(train_df['article']),\n",
    "                                            specials = ['<unk>', '<pad>', '<bos>', '<eos>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_default_index of the vocab to unknown tag\n",
    "vocab_transform.set_default_index(vocab_transform[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1678944418675,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "1Y6fdTXnJ1Tw",
    "outputId": "ccf45371-71e1-4af0-b21e-146564c0dae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30624"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "LV_iXedYJ1Tw",
    "outputId": "d9bd9fa9-66ae-44e7-8b59-1d1aac96b26d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'around'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "mapping = vocab_transform.get_itos()\n",
    "mapping[200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader (1 point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuRbou_kJ1Tw"
   },
   "source": [
    "###  FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "1MwPAaWYJ1Tw"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab_transform.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1678944418676,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "dMRRB9BpJ1Tw",
    "outputId": "23611ef6-01bc-47fc-c780-1a92c9bed749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30624, 300])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "XyVLlnpzJ1Tw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def sequencetial_transforms(*transforms):\n",
    "    #code here\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids):\n",
    "    #code here\n",
    "    return torch.cat((torch.tensor([SOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = {}\n",
    "text_transform = sequencetial_transforms(tokenizer,\n",
    "                                        vocab_transform,\n",
    "                                        tensor_transform)\n",
    "def collate_batch(batch): \n",
    "    #code here\n",
    "    pass\n",
    "    # src_batch, src_len_batch,  trg_batch = [], [], []\n",
    "    \n",
    "    # for src_sample, trg_sample in batch:\n",
    "    #     processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "    #     src_batch.append(processed_text)\n",
    "    #     trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "    #     src_len_batch.append(processed_text.size(0))\n",
    "        \n",
    "    # src_batch = pad_sequence(src_batch, padding_value = PAD_IDX)\n",
    "    # trg_batch = pad_sequence(trg_batch, padding_value = PAD_IDX)\n",
    "    # return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "QOXDjG7vJ1Tx"
   },
   "outputs": [],
   "source": [
    "class DataWrap(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "PfHuFsRVdfe9",
    "outputId": "3fd6a86a-7393-428d-e5e6-9b72d57e71af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       london, england harry potter star daniel radcl...\n",
       "highlights    harry potter star daniel radcliffe gets £20m f...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "o7MCMZFnJ1Tx"
   },
   "outputs": [],
   "source": [
    "train = DataWrap(train_df)\n",
    "valid = DataWrap(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article       london, england harry potter star daniel radcl...\n",
       "highlights    harry potter star daniel radcliffe gets £20m f...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london, england harry potter star daniel radcliffe gains access to a reported £20 million fortune as he turns 18 on monday, but he insists the money won\\'t cast a spell on him. daniel radcliffe as harry potter in \"harry potter and the order of the phoenix\" to the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"i don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an australian interviewer earlier this month. \"i don\\'t think i\\'ll be particularly extravagant. \"the things i like buying are things that cost about 10 pounds books and cds and dvds.\" at 18, radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"hostel: part ii,\" currently six places below his number one movie on the uk box office chart. details of how he\\'ll mark his landmark birthday are under wraps. his agent and publicist had no comment on his plans. \"i\\'ll definitely have some sort of party,\" he said in an interview. \"hopefully none of you will be reading about it.\" radcliffe\\'s earnings from the first five potter films have been held in a trust fund which he has not been able to touch. despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"people are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"but i try very hard not to go that way because it would be too easy for them.\" his latest outing as the boy wizard in \"harry potter and the order of the phoenix\" is breaking records on both sides of the atlantic and he will reprise the role in the last two films. watch ireporter give her review of potter\\'s latest » . there is life beyond potter, however. the londoner has filmed a tv movie called \"my boy jack,\" about author rudyard kipling and his son, due for release later this year. he will also appear in \"december boys,\" an australian film about four boys who escape an orphanage. earlier this year, he made his stage debut playing a tortured teenager in peter shaffer\\'s \"equus.\" meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"i just think i\\'m going to be more sort of fair game,\" he told reuters. email to a friend . copyright 2007 reuters. all rights reserved.this material may not be published, broadcast, rewritten, or redistributed.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0] # article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:978\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    975\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "train[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678944418677,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "pnJjF7y_J1Tx"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678944418678,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "y8wdddWQdV8D",
    "outputId": "f8bda618-1b9f-4557-bc03-acbc619a4264"
   },
   "outputs": [],
   "source": [
    "for src, _, tgt in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src.shape, tgt.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gAkAZOiWJ1Tx"
   },
   "source": [
    "## Create a model  (2 points)\n",
    "- Create a Seq2Seq attention model  \n",
    "- The Attention shoud be Multiplicative Attention  not Additive Attention\n",
    "- The main model must take \"target\" as optional. So that we dont have to pass target in inference.\n",
    "Also include max summary length, teacher forcing etc. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        #embedding\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #packed\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "        #rnn\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #unpacked\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #-1, -2 hidden state\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim = 1)))\n",
    "        \n",
    "        #outputs: [src len, batch_size, hid dim * 2]\n",
    "        #hidden:  [batch_size, hid_dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention  = attention\n",
    "        self.embedding  = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn        = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc         = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        #input: [batch_size]\n",
    "        #hidden: [batch_size, hid_dim]\n",
    "        #encoder_ouputs: [src len, batch_size, hid_dim * 2]\n",
    "        #mask: [batch_size, src len]\n",
    "                \n",
    "        #embed our input\n",
    "        input    = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch_size, emb_dim]\n",
    "\n",
    "        #calculate the attention\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch_size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch_size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_ouputs: [batch_size, src len, hid_dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted: [batch_size, 1, hid_dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted: [1, batch_size, hid_dim * 2]\n",
    "        \n",
    "        #send the input to decoder rnn\n",
    "        #concatenate (embed, weighted encoder_outputs)\n",
    "        #[1, batch_size, emb_dim]; [1, batch_size, hid_dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input: [1, batch_size, emb_dim + hid_dim * 2]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "            \n",
    "        #send the output of the decoder rnn to fc layer to predict the word\n",
    "        #prediction = fc(concatenate (output, weighted, embed))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc(torch.cat((embedded, output, weighted), dim = 1))\n",
    "        #prediction: [batch_size, output_dim]\n",
    "            \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention shoud be Multiplicative Attention not Additive Attention\n",
    "\n",
    "Multiplicative Attention : \n",
    "\n",
    "$e_i = s^T W h_i \\in \\mathbb{R} ; \\textbf{W} \\in \\mathbb{R}^{d_2 × d_1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim, hid_dim)     # for decoder input_\n",
    "        # self.U = nn.Linear(hid_dim * 2, hid_dim) # for encoder outputs\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden = [batch_size, hid_dim] ==> first hidden is basically the last hidden of the encoder\n",
    "        # encoder_outputs = [src_len, batch_size, hid_dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len    = encoder_outputs.shape[0]\n",
    "        \n",
    "        # repeat the hidden src len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden = [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # permute the encoder_outputs so that they can perform multiplication / addition\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        # encoder_outputs = [batch_size, src_len, dim * 2]\n",
    "        \n",
    "        # addition\n",
    "        energy = self.W(hidden) * encoder_outputs.squeeze(2)\n",
    "        energy = torch.matmul(encoder_outputs, self.W(hidden)).squeeze(2)\n",
    "        # [batch_size, src_len, 1] ==> [batch_size, src_len]\n",
    "        \n",
    "        # mask\n",
    "        energy = energy.masked_fill(mask, -1e10)\n",
    "        \n",
    "        # return F.softmax(attention, dim = 1)\n",
    "        return F.softmax(energy, dim = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main model must take \"target\" as optional. So that we dont have to pass target in inference. Also include max summary length, teacher forcing etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device  = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch_size]\n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)  #permute so that it's the same shape as attention\n",
    "        #mask: [batch_size, src len] #(0, 0, 0, 0, 0, 1, 1)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, src_len, teacher_forcing_ratio=0.5, max_length = 10, trg=None):\n",
    "        #src: [src len, batch_size]\n",
    "        #trg: [trg len, batch_size]\n",
    "        \n",
    "        #initialize something\n",
    "        batch_size     = src.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # trg_len        = trg.shape[0]\n",
    "        sum_len        = max_length \n",
    "        \n",
    "        outputs    = torch.zeros().to(self.device) #code here \n",
    "        attentions = torch.zeros().to(self.device) #code here\n",
    "\n",
    "        #send our src text into encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len) \n",
    "        #encoder_outputs refer to all hidden states (last layer)\n",
    "        #hidden refer to the last hidden state (of each layer, of each direction)\n",
    "        \n",
    "        if trg is not None:\n",
    "          input_ = trg[0, :]\n",
    "        else:\n",
    "          input_ = trg[0, :] # need to test\n",
    "            \n",
    "        mask = self.create_mask(src)\n",
    "        for t in range(1, sum_len):\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output: [batch_size, output_dim] ==> predictions\n",
    "            #hidden: [batch_size, hid_dim]\n",
    "            #attention: [batch_size, src len]\n",
    "\n",
    "            #append the output to a list\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1          = output.argmax(1)  #autoregressive\n",
    "            \n",
    "            if trg is not None: \n",
    "              input_ = trg[t] if teacher_force else top1\n",
    "            else:\n",
    "              input_ = trg[t] #code here\n",
    "\n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944418679,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "mbLE5jfOJ1Tx"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_xpjYUW6J1Ty"
   },
   "source": [
    "### Quick Note: Apply the fasttext embedding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2175,
     "status": "ok",
     "timestamp": 1678944420845,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "-9R4rZhQJ1Ty",
    "outputId": "146d89a9-0612-440c-c61f-4a3ce4a37f4a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dim = len(vocab_transform)\n",
    "output_dim = len(vocab_transform)\n",
    "emb_dim =  300\n",
    "hid_dim =  300\n",
    "dropout = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = MultiplicativeAttention(hid_dim, emb_dim)\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "#Applying FastText embedding to the Encoder and the Decoder\n",
    "#\n",
    "\n",
    "model = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "lHENzAP1J1Ty",
    "outputId": "0e3679ae-ea0f-4601-ca98-1f2b4d9c6908"
   },
   "outputs": [],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "X1vODmbwJ1Ty"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Ploting (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678944420846,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "NC94QbkVJ1Ty"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    #code here\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678944420847,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "6Qdbtk5-J1Ty"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    #code here\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1678944424294,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "yXtBAVijJ1Ty"
   },
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader))) \n",
    "valid_loader_length = len(list(iter(valid_loader))) \n",
    "train_loader_length, valid_loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678944424295,
     "user": {
      "displayName": "Amanda Raj Shrestha",
      "userId": "18081018054455298661"
     },
     "user_tz": -420
    },
    "id": "pRc1II-LJ1Ty"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsHnh_zJJ1Tz",
    "outputId": "2ee89313-025d-4807-b21e-a37681aed310"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZORXcgSRJ1Tz"
   },
   "source": [
    "### Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arYG5DCoJ1Tz"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GcKzdjSdJ1Tz"
   },
   "source": [
    "## Finally we can summarize (1 points)\n",
    "- Summarize the given sample (The max length of summary should be 20 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "-zYEaQKzJ1Tz"
   },
   "outputs": [],
   "source": [
    "###Summarize the given sample\n",
    "max_len = 20\n",
    "sample = \"\"\"Mrs Kopta, who has dementia, has been living in a nursing home after she was taken in as a person in need seven years after she disappeared. \n",
    "It's a relief she hasn't been murdered. Her husband Bob Kopta said he had been married to Mrs Kopta for 20 years before she went missing. \n",
    "He said: It's a relief knowing that she's not laying in a ditch somewhere, or murdered somewhere. \n",
    "The 86-year-old added that his family suspected she may be in Puerto Rico but she was declared dead around 25 years ago. \n",
    "The retired electrician also said he had consulted with a psychic about her whereabouts. \n",
    "Mrs Kopta has two sisters - a twin, who died six years ago, and a younger sister who was relieved to learn she's still alive, Mr Kopta added. \n",
    "He said he has experienced a range of emotions over the years but is content knowing his wife is alive and being cared for. \n",
    "After 30 years, you try to forget about it. Now I can forget about it. \n",
    "We know what happened, and she is taken care of now, he said.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case\n",
    "#clean text as well\n",
    "sample = sample.lower()\n",
    "sample = data_cleaning(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"mrs kopta, who has dementia, has been living in a nursing home after she was taken in as a person in need seven years after she disappeared. it's a relief she hasn't been murdered. her husband bob kopta said he had been married to mrs kopta for 20 years before she went missing. he said: it's a relief knowing that she's not laying in a ditch somewhere, or murdered somewhere. the 86yearold added that his family suspected she may be in puerto rico but she was declared dead around 25 years ago. the retired electrician also said he had consulted with a psychic about her whereabouts. mrs kopta has two sisters a twin, who died six years ago, and a younger sister who was relieved to learn she's still alive, mr kopta added. he said he has experienced a range of emotions over the years but is content knowing his wife is alive and being cared for. after 30 years, you try to forget about it. now i can forget about it. we know what happened, and she is taken care of now, he said.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = text_transform(sample).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([213])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, text_length, trg_text, 0) #turn off teacher forcing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pqhRJ-eCJ1Tz"
   },
   "source": [
    "## Conclusion (2 points)\n",
    "- How did the model perform?\n",
    "\n",
    "- Does using the FastText Embedding improve the performance?\n",
    "    - The main advantage of FastText embeddings over Word2Vec is to take into account the internal structure of words while learning word representations, which could be very useful for morphologically rich languages, and also for words that occur rarely.\n",
    "\n",
    "- What do you suggest we can do to increase the performance in text summarization?\n",
    "    - prepare dataset well and increase training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "025113dc59724811bfb88724bb8edd43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d970695aff445b29ef9402fc8b64936",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14578b0e69b9495ab871767777b0d790",
      "value": 3
     }
    },
    "035556ace0094a1db07bb17e0fd170ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14578b0e69b9495ab871767777b0d790": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d970695aff445b29ef9402fc8b64936": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4243f7f3c9f54b66b659911728d5316f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aeeb7e2754454060bfe83a5908d94600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1573faf199443d192e2d5f996054616",
       "IPY_MODEL_025113dc59724811bfb88724bb8edd43",
       "IPY_MODEL_b45dc527e44c45808d627a7382055aae"
      ],
      "layout": "IPY_MODEL_dbc27640827b498d9de76829b0ca911c"
     }
    },
    "b45dc527e44c45808d627a7382055aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7e910835d584f69b72443d20b662e1b",
      "placeholder": "​",
      "style": "IPY_MODEL_4243f7f3c9f54b66b659911728d5316f",
      "value": " 3/3 [00:00&lt;00:00, 95.45it/s]"
     }
    },
    "b7e910835d584f69b72443d20b662e1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bafd0076a9344677b4d21e66153c1394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1573faf199443d192e2d5f996054616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bafd0076a9344677b4d21e66153c1394",
      "placeholder": "​",
      "style": "IPY_MODEL_035556ace0094a1db07bb17e0fd170ce",
      "value": "100%"
     }
    },
    "dbc27640827b498d9de76829b0ca911c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
